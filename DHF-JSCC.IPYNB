{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61624484",
   "metadata": {},
   "source": [
    "# DHF-JSCC Training and Testing on KITTI Dataset üìö\n",
    "\n",
    "**What is DHF-JSCC?** \n",
    "DHF-JSCC (Deep Hierarchical Feature Joint Source-Channel Coding) is a deep learning approach for image compression that jointly optimizes source coding (compression) and channel coding (transmission) using stereo image pairs.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- üéØ Understand joint source-channel coding for image compression\n",
    "- üß† Learn how deep neural networks compress stereo images\n",
    "- üìä Explore rate-distortion optimization in deep learning\n",
    "- üîß Practice training compression models from scratch\n",
    "- üìà Analyze compression performance metrics (PSNR, MS-SSIM, BPP)\n",
    "\n",
    "**What You'll Build:** A complete stereo image compression system that learns to:\n",
    "1. Extract hierarchical features from stereo image pairs\n",
    "2. Compress left images using side information from right images\n",
    "3. Optimize the trade-off between compression ratio and image quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0d995",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries üìö\n",
    "\n",
    "**Why This Step Matters:**\n",
    "Before building any deep learning model, we need to import the right tools. Each library serves a specific purpose in our compression pipeline:\n",
    "\n",
    "- **PyTorch**: Our deep learning framework for building and training neural networks\n",
    "- **Dataset Classes**: Custom loaders for KITTI stereo image pairs\n",
    "- **Compression Modules**: Our DHF-JSCC model architecture\n",
    "- **Loss Functions**: MS-SSIM for perceptual quality measurement\n",
    "- **Visualization**: Tools to monitor training progress and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d5fbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PairKitti imported successfully\n",
      "‚úÖ InStereo2K imported successfully\n",
      "‚úÖ model_d_fusion2 imported successfully\n",
      "\n",
      "Library import summary:\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA devices: 2\n",
      "Current device: NVIDIA GeForce RTX 2080 Ti\n",
      "‚úÖ Core libraries imported successfully!\n",
      "‚úÖ model_d_fusion2 imported successfully\n",
      "\n",
      "Library import summary:\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA devices: 2\n",
      "Current device: NVIDIA GeForce RTX 2080 Ti\n",
      "‚úÖ Core libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current directory to Python path for local imports\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "\n",
    "def import_or_install(package, import_name=None, is_local=False):\n",
    "    try:\n",
    "        if import_name is None:\n",
    "            import_name = package\n",
    "        return importlib.import_module(import_name)\n",
    "    except ImportError:\n",
    "        if is_local:\n",
    "            # For local modules, just re-raise the ImportError with helpful message\n",
    "            print(f\"Local module '{package}' not found. Please ensure the file exists in the current directory.\")\n",
    "            raise ImportError(f\"Local module '{package}' not found\")\n",
    "        else:\n",
    "            # For pip packages, try to install\n",
    "            print(f\"Package '{package}' not found. Installing...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            return importlib.import_module(import_name if import_name else package)\n",
    "\n",
    "# Standard library and pip packages\n",
    "os = import_or_install('os')\n",
    "np = import_or_install('numpy', 'numpy')\n",
    "pd = import_or_install('pandas', 'pandas')\n",
    "torch = import_or_install('torch', 'torch')\n",
    "yaml = import_or_install('pyyaml', 'yaml')\n",
    "Image = import_or_install('Pillow', 'PIL.Image')\n",
    "OrderedDict = import_or_install('collections', 'collections').OrderedDict\n",
    "DataLoader = import_or_install('torch', 'torch.utils.data').DataLoader\n",
    "ms_ssim = import_or_install('pytorch-msssim', 'pytorch_msssim').ms_ssim\n",
    "math = import_or_install('math')\n",
    "plt = import_or_install('matplotlib', 'matplotlib.pyplot')\n",
    "display = import_or_install('IPython', 'IPython.display').display\n",
    "\n",
    "# Local modules - use direct import with importlib\n",
    "try:\n",
    "    # Clear any cached imports\n",
    "    if 'dataset' in sys.modules:\n",
    "        importlib.reload(sys.modules['dataset'])\n",
    "    if 'dataset.PairKitti' in sys.modules:\n",
    "        importlib.reload(sys.modules['dataset.PairKitti'])\n",
    "    \n",
    "    # Import the module\n",
    "    dataset_module = importlib.import_module('dataset.PairKitti')\n",
    "    PairKitti = dataset_module.PairKitti\n",
    "    print(\"‚úÖ PairKitti imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not import PairKitti: {e}\")\n",
    "    print(f\"Trying direct sys.path approach...\")\n",
    "    try:\n",
    "        # Alternative: add dataset folder to path and import directly\n",
    "        dataset_path = os.path.join(os.getcwd(), 'dataset')\n",
    "        if dataset_path not in sys.path:\n",
    "            sys.path.insert(0, dataset_path)\n",
    "        from PairKitti import PairKitti\n",
    "        print(\"‚úÖ PairKitti imported successfully (direct import)\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Direct import also failed: {e2}\")\n",
    "        PairKitti = None\n",
    "\n",
    "try:\n",
    "    dataset_module = importlib.import_module('dataset.InStereo2K')\n",
    "    InStereo2K = dataset_module.InStereo2K\n",
    "    print(\"‚úÖ InStereo2K imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not import InStereo2K: {e}\")\n",
    "    try:\n",
    "        from InStereo2K import InStereo2K\n",
    "        print(\"‚úÖ InStereo2K imported successfully (direct import)\")\n",
    "    except:\n",
    "        InStereo2K = None\n",
    "\n",
    "try:\n",
    "    import model_d_fusion2\n",
    "    print(\"‚úÖ model_d_fusion2 imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Could not import model_d_fusion2: {e}\")\n",
    "    print(\"This might be due to missing dependencies in the local module.\")\n",
    "    model_d_fusion2 = None\n",
    "\n",
    "print(\"\\nLibrary import summary:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"‚úÖ Core libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5896e441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CUDA AND PYTORCH INSTALLATION CHECK\n",
      "============================================================\n",
      "Current PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "Number of CUDA devices: 2\n",
      "Current device: NVIDIA GeForce RTX 2080 Ti\n",
      "‚úÖ CUDA is properly configured!\n",
      "\n",
      "============================================================\n",
      "INSTALLING OTHER DEPENDENCIES\n",
      "============================================================\n",
      "‚úÖ torchvision already installed\n",
      "‚úÖ pytorch-msssim already installed\n",
      "Installing Pillow...\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (11.3.0)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (11.3.0)\n",
      "Installing pyyaml...\n",
      "Installing pyyaml...\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (6.0.3)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (6.0.3)\n",
      "‚úÖ pandas already installed\n",
      "‚úÖ matplotlib already installed\n",
      "‚úÖ numpy already installed\n",
      "\n",
      "‚úÖ All dependencies check complete!\n",
      "You may now re-run the import cell above.\n",
      "‚úÖ pandas already installed\n",
      "‚úÖ matplotlib already installed\n",
      "‚úÖ numpy already installed\n",
      "\n",
      "‚úÖ All dependencies check complete!\n",
      "You may now re-run the import cell above.\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability and install PyTorch with CUDA if needed\n",
    "import subprocess\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "def check_cuda_and_install():\n",
    "    \"\"\"Check CUDA availability and install appropriate PyTorch version\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CUDA AND PYTORCH INSTALLATION CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if CUDA is available with current PyTorch installation\n",
    "    print(f\"Current PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "        if torch.cuda.device_count() > 0:\n",
    "            print(f\"Current device: {torch.cuda.get_device_name()}\")\n",
    "        print(\"‚úÖ CUDA is properly configured!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå CUDA is not available with current PyTorch installation\")\n",
    "        \n",
    "        # Check if NVIDIA GPU is available on system\n",
    "        try:\n",
    "            result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)\n",
    "            if result.returncode == 0:\n",
    "                print(\"‚úÖ NVIDIA GPU detected on system\")\n",
    "                print(\"Installing PyTorch with CUDA support...\")\n",
    "                \n",
    "                # Install PyTorch with CUDA support\n",
    "                install_commands = [\n",
    "                    [sys.executable, \"-m\", \"pip\", \"uninstall\", \"torch\", \"torchvision\", \"torchaudio\", \"-y\"],\n",
    "                    [sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cu121\"]\n",
    "                ]\n",
    "                \n",
    "                for cmd in install_commands:\n",
    "                    print(f\"Running: {' '.join(cmd)}\")\n",
    "                    subprocess.check_call(cmd)\n",
    "                \n",
    "                # Restart kernel notification\n",
    "                print(\"\\n‚ö†Ô∏è  IMPORTANT: You may need to restart the kernel for changes to take effect.\")\n",
    "                print(\"   Go to Kernel -> Restart Kernel to restart.\")\n",
    "                \n",
    "                return True\n",
    "            else:\n",
    "                print(\"‚ùå No NVIDIA GPU detected on system\")\n",
    "                print(\"Installing CPU-only PyTorch...\")\n",
    "                \n",
    "                # Install CPU-only version\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"])\n",
    "                return False\n",
    "                \n",
    "        except (subprocess.TimeoutExpired, FileNotFoundError):\n",
    "            print(\"‚ùå nvidia-smi not found - no NVIDIA GPU available\")\n",
    "            print(\"Installing CPU-only PyTorch...\")\n",
    "            \n",
    "            # Install CPU-only version\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"])\n",
    "            return False\n",
    "\n",
    "# Run the check\n",
    "cuda_available = check_cuda_and_install()\n",
    "\n",
    "# Also install other missing dependencies\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INSTALLING OTHER DEPENDENCIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "additional_packages = [\n",
    "    \"torchvision\",  # For image transforms\n",
    "    \"pytorch-msssim\",  # For MS-SSIM loss\n",
    "    \"Pillow\",  # For image processing\n",
    "    \"pyyaml\",  # For config files\n",
    "    \"pandas\",  # For data handling\n",
    "    \"matplotlib\",  # For visualization\n",
    "    \"numpy\"  # For numerical operations\n",
    "]\n",
    "\n",
    "for package in additional_packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_').lower())\n",
    "        print(f\"‚úÖ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies check complete!\")\n",
    "print(\"You may now re-run the import cell above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e82ac818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to install CompressAI...\n",
      "Requirement already satisfied: compressai in ./.venv/lib/python3.12/site-packages (1.2.8)\n",
      "Requirement already satisfied: einops in ./.venv/lib/python3.12/site-packages (from compressai) (0.8.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (from compressai) (3.10.6)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21.0 in ./.venv/lib/python3.12/site-packages (from compressai) (1.26.4)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from compressai) (2.3.3)\n",
      "Requirement already satisfied: pybind11>=2.6.0 in ./.venv/lib/python3.12/site-packages (from compressai) (3.0.1)\n",
      "Requirement already satisfied: pytorch-msssim in ./.venv/lib/python3.12/site-packages (from compressai) (1.0.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from compressai) (1.16.2)\n",
      "Requirement already satisfied: setuptools>=68 in ./.venv/lib/python3.12/site-packages (from compressai) (80.9.0)\n",
      "Requirement already satisfied: tomli>=2.2.1 in ./.venv/lib/python3.12/site-packages (from compressai) (2.2.1)\n",
      "Requirement already satisfied: torch-geometric>=2.3.0 in ./.venv/lib/python3.12/site-packages (from compressai) (2.6.1)\n",
      "Requirement already satisfied: torch>=1.13.1 in ./.venv/lib/python3.12/site-packages (from compressai) (2.8.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (from compressai) (0.23.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from compressai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.12/site-packages (from compressai) (4.15.0)\n",
      "Requirement already satisfied: wheel>=0.32.0 in ./.venv/lib/python3.12/site-packages (from compressai) (0.45.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (3.4.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from torch-geometric>=2.3.0->compressai) (3.13.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in ./.venv/lib/python3.12/site-packages (from torch-geometric>=2.3.0->compressai) (7.1.0)\n",
      "Requirement already satisfied: pyparsing in ./.venv/lib/python3.12/site-packages (from torch-geometric>=2.3.0->compressai) (3.2.5)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from torch-geometric>=2.3.0->compressai) (2.32.5)\n",
      "Requirement already satisfied: compressai in ./.venv/lib/python3.12/site-packages (1.2.8)\n",
      "Requirement already satisfied: einops in ./.venv/lib/python3.12/site-packages (from compressai) (0.8.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (from compressai) (3.10.6)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21.0 in ./.venv/lib/python3.12/site-packages (from compressai) (1.26.4)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from compressai) (2.3.3)\n",
      "Requirement already satisfied: pybind11>=2.6.0 in ./.venv/lib/python3.12/site-packages (from compressai) (3.0.1)\n",
      "Requirement already satisfied: pytorch-msssim in ./.venv/lib/python3.12/site-packages (from compressai) (1.0.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from compressai) (1.16.2)\n",
      "Requirement already satisfied: setuptools>=68 in ./.venv/lib/python3.12/site-packages (from compressai) (80.9.0)\n",
      "Requirement already satisfied: tomli>=2.2.1 in ./.venv/lib/python3.12/site-packages (from compressai) (2.2.1)\n",
      "Requirement already satisfied: torch-geometric>=2.3.0 in ./.venv/lib/python3.12/site-packages (from compressai) (2.6.1)\n",
      "Requirement already satisfied: torch>=1.13.1 in ./.venv/lib/python3.12/site-packages (from compressai) (2.8.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (from compressai) (0.23.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from compressai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.12/site-packages (from compressai) (4.15.0)\n",
      "Requirement already satisfied: wheel>=0.32.0 in ./.venv/lib/python3.12/site-packages (from compressai) (0.45.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.1->compressai) (3.4.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from torch-geometric>=2.3.0->compressai) (3.13.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in ./.venv/lib/python3.12/site-packages (from torch-geometric>=2.3.0->compressai) (7.1.0)\n",
      "Requirement already satisfied: pyparsing in ./.venv/lib/python3.12/site-packages (from torch-geometric>=2.3.0->compressai) (3.2.5)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from torch-geometric>=2.3.0->compressai) (2.32.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (11.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->compressai) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->compressai) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->compressai) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.1->compressai) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (11.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib->compressai) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->compressai) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->compressai) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->compressai) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.1->compressai) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (0.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.13.1->compressai) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->torch-geometric>=2.3.0->compressai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->torch-geometric>=2.3.0->compressai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->torch-geometric>=2.3.0->compressai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->torch-geometric>=2.3.0->compressai) (2025.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (0.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.13.1->compressai) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->torch-geometric>=2.3.0->compressai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->torch-geometric>=2.3.0->compressai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->torch-geometric>=2.3.0->compressai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->torch-geometric>=2.3.0->compressai) (2025.10.5)\n",
      "‚úÖ ops.py created using CompressAI operations\n",
      "‚úÖ model_d_fusion2 imported successfully after ops setup!\n",
      "ops module setup complete!\n",
      "‚úÖ ops.py created using CompressAI operations\n",
      "‚úÖ model_d_fusion2 imported successfully after ops setup!\n",
      "ops module setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Handle missing ops module - try to install CompressAI or create a basic ops module\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def setup_ops_module():\n",
    "    \"\"\"Setup the missing ops module needed by the local modules\"\"\"\n",
    "    \n",
    "    # First try to install CompressAI which contains common compression operations\n",
    "    try:\n",
    "        print(\"Trying to install CompressAI...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"compressai\"])\n",
    "        \n",
    "        # Try to import operations from CompressAI\n",
    "        try:\n",
    "            from compressai.ops import LowerBound\n",
    "            from compressai.layers import GDN\n",
    "            \n",
    "            # Create a simple ops.py file that imports from CompressAI\n",
    "            ops_content = '''# Auto-generated ops module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from compressai.ops import LowerBound\n",
    "from compressai.layers import GDN\n",
    "\n",
    "class Low_bound(torch.autograd.Function):\n",
    "    \"\"\"Lower bound operation\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, bound):\n",
    "        return LowerBound.apply(x, bound)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None\n",
    "\n",
    "# Export GDN from CompressAI\n",
    "GDN = GDN\n",
    "'''\n",
    "            \n",
    "            with open('ops.py', 'w') as f:\n",
    "                f.write(ops_content)\n",
    "            \n",
    "            print(\"‚úÖ ops.py created using CompressAI operations\")\n",
    "            return True\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"‚ùå CompressAI installed but couldn't import required operations\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CompressAI installation failed: {e}\")\n",
    "    \n",
    "    # If CompressAI doesn't work, create a basic ops module\n",
    "    print(\"Creating basic ops module...\")\n",
    "    \n",
    "    basic_ops_content = '''# Basic ops module for DHF-JSCC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Low_bound(torch.autograd.Function):\n",
    "    \"\"\"Lower bound operation - clamps values to minimum bound\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, bound):\n",
    "        ctx.save_for_backward(x)\n",
    "        ctx.bound = bound\n",
    "        return torch.clamp(x, min=bound)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[x < ctx.bound] = 0\n",
    "        return grad_input, None\n",
    "\n",
    "class GDN(nn.Module):\n",
    "    \"\"\"Generalized Divisive Normalization (GDN) layer\"\"\"\n",
    "    \n",
    "    def __init__(self, channels, inverse=False, beta_min=1e-6, gamma_init=0.1):\n",
    "        super().__init__()\n",
    "        self.inverse = inverse\n",
    "        self.beta_min = beta_min\n",
    "        \n",
    "        self.beta = nn.Parameter(torch.ones(channels))\n",
    "        self.gamma = nn.Parameter(gamma_init * torch.eye(channels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.inverse:\n",
    "            # Inverse GDN\n",
    "            beta = torch.clamp(self.beta, min=self.beta_min)\n",
    "            gamma = torch.abs(self.gamma)\n",
    "            norm = torch.tensordot(x**2, gamma, dims=([1], [0])) + beta\n",
    "            return x * torch.sqrt(norm)\n",
    "        else:\n",
    "            # Forward GDN  \n",
    "            beta = torch.clamp(self.beta, min=self.beta_min)\n",
    "            gamma = torch.abs(self.gamma)\n",
    "            norm = torch.tensordot(x**2, gamma, dims=([1], [0])) + beta\n",
    "            return x / torch.sqrt(norm)\n",
    "'''\n",
    "    \n",
    "    with open('ops.py', 'w') as f:\n",
    "        f.write(basic_ops_content)\n",
    "    \n",
    "    print(\"‚úÖ Basic ops.py created with Low_bound and GDN operations\")\n",
    "    return True\n",
    "\n",
    "# Setup ops module\n",
    "setup_ops_module()\n",
    "\n",
    "# Now try to import the local modules again\n",
    "try:\n",
    "    import model_d_fusion2\n",
    "    print(\"‚úÖ model_d_fusion2 imported successfully after ops setup!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Still couldn't import model_d_fusion2: {e}\")\n",
    "    print(\"There might be other missing dependencies.\")\n",
    "\n",
    "print(\"ops module setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac878a",
   "metadata": {},
   "source": [
    "## ‚úÖ CUDA and Dependencies Status\n",
    "\n",
    "**CUDA Configuration:**\n",
    "- ‚úÖ CUDA is properly installed and configured\n",
    "- ‚úÖ PyTorch version: 2.8.0+cu128  \n",
    "- ‚úÖ CUDA version: 12.8\n",
    "- ‚úÖ GPU devices available: 2 (including NVIDIA GeForce RTX 2080 Ti)\n",
    "\n",
    "**Dependencies Status:**\n",
    "- ‚úÖ All core libraries imported successfully\n",
    "- ‚úÖ Local dataset modules (PairKitti, InStereo2K) working\n",
    "- ‚úÖ Model modules (model_d_fusion2) working after ops module creation\n",
    "- ‚úÖ CompressAI or custom ops module installed for compression operations\n",
    "\n",
    "**Ready for Training:**\n",
    "The environment is now properly configured for DHF-JSCC training with CUDA acceleration on your RTX 2080 Ti GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f907e5",
   "metadata": {},
   "source": [
    "## üéì Understanding the Complete DHF-JSCC Pipeline\n",
    "\n",
    "**What Just Happened? A Step-by-Step Breakdown:**\n",
    "\n",
    "### üß† **The Science Behind It:**\n",
    "DHF-JSCC combines several advanced concepts:\n",
    "1. **Deep Learning**: Neural networks learn compression patterns from data\n",
    "2. **Stereo Vision**: Uses geometric relationships between camera views\n",
    "3. **Rate-Distortion Theory**: Optimal trade-off between file size and quality\n",
    "4. **Entropy Coding**: Efficient representation based on probability distributions\n",
    "\n",
    "### üîÑ **The Training Process:**\n",
    "```\n",
    "Raw Stereo Images ‚Üí Neural Network ‚Üí Compressed Representation ‚Üí Reconstructed Images\n",
    "      ‚Üë                    ‚Üì                                           ‚Üì\n",
    "   Dataset            Feature Learning                            Quality Measurement\n",
    "                           ‚Üì                                           ‚Üì\n",
    "                    Weight Updates ‚Üê Backpropagation ‚Üê Loss Calculation\n",
    "```\n",
    "\n",
    "### üìä **Key Innovations:**\n",
    "- **Joint Source-Channel Coding**: Optimizes compression AND transmission together\n",
    "- **Side Information**: Right image helps compress left image more efficiently  \n",
    "- **Hierarchical Features**: Multi-scale processing captures both details and structure\n",
    "- **Learned Entropy Models**: Neural networks estimate probability better than traditional methods\n",
    "\n",
    "### üéØ **Why This Matters:**\n",
    "- **Practical**: Better compression for autonomous vehicles, VR/AR, streaming\n",
    "- **Academic**: Advances our understanding of neural compression\n",
    "- **Technical**: Shows how to combine computer vision + information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9e368",
   "metadata": {},
   "source": [
    "## 2. Configuration Settings ‚öôÔ∏è\n",
    "\n",
    "**Understanding the Hyperparameters:**\n",
    "Each setting controls a different aspect of our compression model. Let's understand why each matters:\n",
    "\n",
    "**Dataset Parameters:**\n",
    "- `resize`: Smaller images (128x128) train faster but may lose detail\n",
    "- `dataset_name`: KITTI provides real-world stereo driving scenes\n",
    "\n",
    "**Model Architecture:**\n",
    "- `baseline_model`: 'bls17' refers to Ball√© 2017 entropy model\n",
    "- `use_side_info`: Using right image to help compress left image\n",
    "- `num_filters`: More filters = more capacity but slower training\n",
    "\n",
    "**Training Strategy:**\n",
    "- `lambda`: Controls rate-distortion trade-off (higher = more compression)\n",
    "- `lr`: Learning rate (too high = unstable, too low = slow)\n",
    "- `distortion_loss`: MSE vs MS-SSIM (perceptual quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0672196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration for training from scratch:\n",
      "  Dataset: KITTI (stereo driving scenes)\n",
      "  Image size: [128, 128] (resize for speed)\n",
      "  Using CUDA: True (üöÄ GPU acceleration)\n",
      "  GPUs available: 2\n",
      "    ‚Ä¢ GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "    ‚Ä¢ GPU 1: NVIDIA GeForce RTX 2080 Ti\n",
      "  Multi-GPU: ‚úÖ ENABLED (using 2 GPUs with DataParallel)\n",
      "  Load pretrained weights: False (training from scratch)\n",
      "  Training epochs: 30,000 iterations\n",
      "  Batch size: 12 images per batch\n",
      "  Learning rate: 0.0001 (Adam optimizer)\n",
      "  Lambda (rate-distortion): 3e-05 (compression priority)\n",
      "  Distortion loss: MSE (quality metric)\n",
      "  Verbose period: 50 epochs between updates\n",
      "‚úÖ Ready for training from scratch!\n"
     ]
    }
   ],
   "source": [
    "# Configuration for KITTI dataset - Training from Scratch\n",
    "config = {\n",
    "    # ===========================================\n",
    "    # üì∏ DATASET PARAMETERS\n",
    "    # ===========================================\n",
    "    'dataset_name': 'KITTI',  # Real-world stereo driving scenes\n",
    "    'dataset_path': '.',      # Current directory contains dataset folder\n",
    "    'resize': [128, 128],     # Smaller size for faster training (can increase later)\n",
    "    \n",
    "    # ===========================================\n",
    "    # üß† MODEL ARCHITECTURE PARAMETERS  \n",
    "    # ===========================================\n",
    "    'baseline_model': 'bls17',    # Ball√© 2017 entropy model (proven architecture)\n",
    "    'use_side_info': True,        # Use right image to help compress left image\n",
    "    'num_filters': 256,           # Network capacity (more = better quality, slower training)\n",
    "    'cuda': torch.cuda.is_available(),  # Auto-detect GPU availability\n",
    "    'multi_gpu': True,            # üÜï Enable multi-GPU training (DataParallel)\n",
    "    \n",
    "    # ===========================================\n",
    "    # üíæ PRETRAINED WEIGHTS (disabled for scratch training)\n",
    "    # ===========================================\n",
    "    'load_weight': False,         # Set to True if you have pretrained weights\n",
    "    'weight_path': './pretrained_weights/ours+balle17_MS-SSIM_lambda3e-05.pt',\n",
    "    \n",
    "    # ===========================================\n",
    "    # üéì TRAINING HYPERPARAMETERS\n",
    "    # ===========================================\n",
    "    'train': True,               # Enable training mode\n",
    "    'epochs': 30000,             # üÜï Number of training iterations (30K EPOCHS!)\n",
    "    'train_batch_size': 12,      # üÜï Images per batch (increased for 2 GPUs)\n",
    "    'lr': 0.0001,                # Learning rate (Adam optimizer default)\n",
    "    \n",
    "    # Rate-Distortion Trade-off Parameters:\n",
    "    'lambda': 0.00003,           # üéØ KEY PARAMETER: Controls compression vs quality\n",
    "                                 # Higher Œª = more compression, lower quality\n",
    "                                 # Lower Œª = less compression, higher quality\n",
    "    'alpha': 1,                  # Weight for side information loss\n",
    "    'beta': 1,                   # Weight for additional entropy terms\n",
    "    \n",
    "    # Loss Function Choice:\n",
    "    'distortion_loss': 'MSE',    # MSE = fast but less perceptual\n",
    "                                 # MS-SSIM = slower but more realistic\n",
    "    'verbose_period': 50,        # üÜï Print progress every 50 epochs (more frequent for long training)\n",
    "    \n",
    "    # ===========================================\n",
    "    # üíæ SAVING AND OUTPUT PARAMETERS\n",
    "    # ===========================================\n",
    "    'save_weights': True,        # Automatically save best model\n",
    "    'save_output_path': './outputs',  # Where to save results\n",
    "    'experiment_name': 'from_scratch_bls17_MSE_lambda3e-05',\n",
    "    \n",
    "    # ===========================================\n",
    "    # üß™ TESTING PARAMETERS\n",
    "    # ===========================================\n",
    "    'test': True,                # Run evaluation after training\n",
    "    'save_image': True           # Save reconstructed images for visual inspection\n",
    "}\n",
    "\n",
    "# üìä CONFIGURATION SUMMARY\n",
    "print(\"Configuration for training from scratch:\")\n",
    "print(f\"  Dataset: {config['dataset_name']} (stereo driving scenes)\")\n",
    "print(f\"  Image size: {config['resize']} (resize for speed)\")\n",
    "print(f\"  Using CUDA: {config['cuda']} ({'üöÄ GPU acceleration' if config['cuda'] else 'üêå CPU only'})\")\n",
    "if config['cuda']:\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"  GPUs available: {gpu_count}\")\n",
    "    for i in range(gpu_count):\n",
    "        print(f\"    ‚Ä¢ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    if config['multi_gpu'] and gpu_count > 1:\n",
    "        print(f\"  Multi-GPU: ‚úÖ ENABLED (using {gpu_count} GPUs with DataParallel)\")\n",
    "    else:\n",
    "        print(f\"  Multi-GPU: ‚ùå Disabled (using single GPU)\")\n",
    "print(f\"  Load pretrained weights: {config['load_weight']} (training from scratch)\")\n",
    "print(f\"  Training epochs: {config['epochs']:,} iterations\")\n",
    "print(f\"  Batch size: {config['train_batch_size']} images per batch\")\n",
    "print(f\"  Learning rate: {config['lr']} (Adam optimizer)\")\n",
    "print(f\"  Lambda (rate-distortion): {config['lambda']} (compression priority)\")\n",
    "print(f\"  Distortion loss: {config['distortion_loss']} (quality metric)\")\n",
    "print(f\"  Verbose period: {config['verbose_period']} epochs between updates\")\n",
    "print(\"‚úÖ Ready for training from scratch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ffbc1",
   "metadata": {},
   "source": [
    "## 3. Helper Functions üîß\n",
    "\n",
    "**Core Compression Metrics:**\n",
    "These functions implement the fundamental metrics used in image compression research:\n",
    "\n",
    "**1. BPP (Bits Per Pixel):**\n",
    "- Measures compression efficiency\n",
    "- Lower BPP = better compression\n",
    "- Calculated from entropy model likelihoods\n",
    "\n",
    "**2. Distortion Functions:**\n",
    "- **MSE**: Pixel-wise differences (simple but not perceptually aligned)\n",
    "- **MS-SSIM**: Multi-scale structural similarity (matches human perception)\n",
    "- Trade-off: MSE is faster, MS-SSIM is more realistic\n",
    "\n",
    "**3. Rate-Distortion Optimization:**\n",
    "- Balance between file size (rate) and image quality (distortion)\n",
    "- Lambda parameter controls this trade-off\n",
    "- Higher lambda = prioritize compression over quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6fdc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Helper functions defined successfully!\n",
      "üîß These functions handle:\n",
      "   ‚Ä¢ BPP calculation (compression efficiency)\n",
      "   ‚Ä¢ Distortion measurement (image quality)\n",
      "   ‚Ä¢ Image saving (visual results)\n",
      "   ‚Ä¢ Weight mapping (model compatibility)\n"
     ]
    }
   ],
   "source": [
    "def get_bpp(model_out, config):\n",
    "    \"\"\"\n",
    "    üìè BITS PER PIXEL (BPP) CALCULATION\n",
    "    \n",
    "    This is the \"rate\" in rate-distortion optimization.\n",
    "    BPP measures compression efficiency - lower is better!\n",
    "    \n",
    "    How it works:\n",
    "    1. Neural networks output probability distributions (likelihoods)\n",
    "    2. We use entropy to estimate bit requirements\n",
    "    3. Different model architectures have different output formats\n",
    "    \n",
    "    Args:\n",
    "        model_out: Output from the compression model\n",
    "        config: Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        bpp: Total bits per pixel (rate)\n",
    "        transmitted_bpp: Actually transmitted bits (excluding side info)\n",
    "    \"\"\"\n",
    "    alpha = config['alpha']  # Weight for side information\n",
    "    beta = config['beta']    # Weight for additional entropy terms\n",
    "    \n",
    "    # üèóÔ∏è Handle different model architectures\n",
    "    if config['baseline_model'] == 'bmshj18':\n",
    "        # Ball√© et al. 2018 - more complex entropy model\n",
    "        if config['use_side_info']:\n",
    "            x_recon, y_recon, likelihoods, y_likelihoods, z_likelihoods, z_likelihoods_cor, w_likelihoods = model_out\n",
    "            size_est = (-np.log(2) * x_recon.numel() / 3)  # Estimate size in bits\n",
    "            \n",
    "            # Main stream bits (actually transmitted)\n",
    "            bpp = (torch.sum(torch.log(likelihoods)) + torch.sum(torch.log(z_likelihoods))) / size_est\n",
    "            transmitted_bpp = bpp.clone().detach()\n",
    "            \n",
    "            # Add side information costs (weighted)\n",
    "            bpp += alpha * (torch.sum(torch.log(y_likelihoods)) + torch.sum(torch.log(z_likelihoods_cor))) / size_est\n",
    "            bpp += beta * torch.sum(torch.log(w_likelihoods)) / size_est\n",
    "            return bpp, transmitted_bpp\n",
    "        else:\n",
    "            # No side information - simpler calculation\n",
    "            x_recon, likelihoods, z_likelihoods = model_out\n",
    "            size_est = (-np.log(2) * x_recon.numel() / 3)\n",
    "            bpp = (torch.sum(torch.log(likelihoods)) + torch.sum(torch.log(z_likelihoods))) / size_est\n",
    "            return bpp, bpp\n",
    "            \n",
    "    elif config['baseline_model'] == 'bls17':\n",
    "        # üéØ Ball√© et al. 2017 - our current model (simpler entropy model)\n",
    "        if config['use_side_info']:\n",
    "            # With side information (stereo compression)\n",
    "            x_recon, y_recon, likelihoods, y_likelihoods, w_likelihoods = model_out\n",
    "            size_est = (-np.log(2) * x_recon.numel() / 3)\n",
    "            \n",
    "            # Main compression stream\n",
    "            bpp = torch.sum(torch.log(likelihoods)) / size_est\n",
    "            transmitted_bpp = bpp.clone().detach()\n",
    "            \n",
    "            # Add weighted side information costs\n",
    "            bpp += alpha * torch.sum(torch.log(y_likelihoods)) / size_est\n",
    "            bpp += beta * torch.sum(torch.log(w_likelihoods)) / size_est\n",
    "            return bpp, transmitted_bpp\n",
    "        else:\n",
    "            # Single image compression (no stereo)\n",
    "            x_recon, likelihoods = model_out\n",
    "            size_est = (-np.log(2) * x_recon.numel() / 3)\n",
    "            bpp = torch.sum(torch.log(likelihoods)) / size_est\n",
    "            return bpp, bpp\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_distortion(config, out_l, out_r, img, cor_img, mse):\n",
    "    \"\"\"\n",
    "    üìê DISTORTION MEASUREMENT\n",
    "    \n",
    "    This is the \"distortion\" in rate-distortion optimization.\n",
    "    Measures how much the reconstructed image differs from original.\n",
    "    \n",
    "    Two main approaches:\n",
    "    - MSE: Simple pixel differences (fast but not perceptually accurate)\n",
    "    - MS-SSIM: Structural similarity (slower but matches human perception)\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        out_l, out_r: Reconstructed left and right images\n",
    "        img, cor_img: Original left and right images\n",
    "        mse: MSE loss function\n",
    "    \n",
    "    Returns:\n",
    "        distortion: Total distortion value (lower is better quality)\n",
    "    \"\"\"\n",
    "    distortion = None\n",
    "    alpha = config['alpha']  # Weight for side information\n",
    "    \n",
    "    if config['use_side_info']:\n",
    "        # üë´ Stereo compression - measure distortion for both images\n",
    "        x_recon, y_recon = out_l, out_r\n",
    "        \n",
    "        if config['distortion_loss'] == 'MS-SSIM':\n",
    "            # üëÅÔ∏è Perceptually-motivated loss (better for human vision)\n",
    "            # MS-SSIM ranges from 0 (worst) to 1 (perfect)\n",
    "            # We use (1 - MS-SSIM) so lower is better\n",
    "            distortion = (1 - ms_ssim(img.cpu(), x_recon.cpu(), data_range=1.0, size_average=True,\n",
    "                                      win_size=7))\n",
    "            distortion += alpha * (1 - ms_ssim(cor_img.cpu(), y_recon.cpu(), data_range=1.0, size_average=True,\n",
    "                                               win_size=7))\n",
    "        elif config['distortion_loss'] == 'MSE':\n",
    "            # üìä Simple pixel-wise differences (faster but less realistic)\n",
    "            distortion = mse(img, x_recon)\n",
    "            distortion += alpha * mse(cor_img, y_recon)\n",
    "    else:\n",
    "        # üë§ Single image compression\n",
    "        x_recon = out_l\n",
    "        if config['distortion_loss'] == 'MS-SSIM':\n",
    "            distortion = (1 - ms_ssim(img.cpu(), x_recon.cpu(), data_range=1.0, size_average=True,\n",
    "                                      win_size=7))\n",
    "        elif config['distortion_loss'] == 'MSE':\n",
    "            distortion = mse(img, x_recon)\n",
    "    return distortion\n",
    "\n",
    "\n",
    "def save_image(x_recon, x, path, name):\n",
    "    \"\"\"\n",
    "    üíæ SAVE RECONSTRUCTED IMAGES\n",
    "    \n",
    "    Saves original and reconstructed images side-by-side for visual comparison.\n",
    "    This helps you see the compression quality visually.\n",
    "    \n",
    "    Args:\n",
    "        x_recon: Reconstructed image tensor\n",
    "        x: Original image tensor  \n",
    "        path: Directory to save images\n",
    "        name: Filename (without extension)\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy arrays and scale to [0, 255]\n",
    "    img_recon = np.clip((x_recon * 255).squeeze().cpu().numpy(), 0, 255)\n",
    "    img = np.clip((x * 255).squeeze().cpu().numpy(), 0, 255)\n",
    "    \n",
    "    # Rearrange dimensions from (C, H, W) to (H, W, C) for PIL\n",
    "    img_recon = np.transpose(img_recon, (1, 2, 0)).astype('uint8')\n",
    "    img = np.transpose(img, (1, 2, 0)).astype('uint8')\n",
    "    \n",
    "    # Concatenate original and reconstructed side-by-side\n",
    "    img_final = Image.fromarray(np.concatenate((img, img_recon), axis=1), 'RGB')\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    # Save the comparison image\n",
    "    img_final.save(os.path.join(path, name + '.png'))\n",
    "\n",
    "\n",
    "def map_layers(weight):\n",
    "    \"\"\"\n",
    "    üîÑ LAYER NAME MAPPING\n",
    "    \n",
    "    Sometimes pretrained weights have different layer names.\n",
    "    This function maps old names to new names for compatibility.\n",
    "    \n",
    "    Args:\n",
    "        weight: Dictionary of model weights\n",
    "    \n",
    "    Returns:\n",
    "        OrderedDict with mapped layer names\n",
    "    \"\"\"\n",
    "    return OrderedDict([(k.replace('z', 'w'), v) if 'z' in k else (k, v) for k, v in weight.items()])\n",
    "\n",
    "\n",
    "print(\"üìö Helper functions defined successfully!\")\n",
    "print(\"üîß These functions handle:\")\n",
    "print(\"   ‚Ä¢ BPP calculation (compression efficiency)\")\n",
    "print(\"   ‚Ä¢ Distortion measurement (image quality)\")\n",
    "print(\"   ‚Ä¢ Image saving (visual results)\")\n",
    "print(\"   ‚Ä¢ Weight mapping (model compatibility)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62fa744",
   "metadata": {},
   "source": [
    "## 4. Dataset Initialization üì∏\n",
    "\n",
    "**Understanding KITTI Stereo Dataset:**\n",
    "- **What**: Real driving scenes from Karlsruhe, Germany\n",
    "- **Why**: Realistic stereo pairs with known geometry\n",
    "- **Structure**: Left/right camera pairs with calibrated disparity\n",
    "- **Use Case**: Perfect for stereo compression research\n",
    "\n",
    "**Data Loading Strategy:**\n",
    "- **Training**: Learn compression patterns (largest split)\n",
    "- **Validation**: Monitor overfitting during training  \n",
    "- **Testing**: Final performance evaluation (never seen during training)\n",
    "\n",
    "**Stereo Compression Advantage:**\n",
    "Using right image as \"side information\" helps compress left image more efficiently because:\n",
    "1. Stereo images share similar content (same scene)\n",
    "2. Geometric relationships provide predictable correlations\n",
    "3. Joint compression exploits redundancy between views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bbbc51",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Dataset Setup Check\n",
    "\n",
    "**Before running the training, we need to verify the KITTI dataset is properly downloaded.**\n",
    "\n",
    "The dataset should contain:\n",
    "- `data_scene_flow_multiview/training/image_2/` (left images)\n",
    "- `data_scene_flow_multiview/training/image_3/` (right images)\n",
    "\n",
    "If you see a FileNotFoundError, it means the actual image files need to be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "583c8e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking KITTI dataset structure...\n",
      "============================================================\n",
      "‚úÖ data_scene_flow_multiview/training/image_2\n",
      "   Found 400 images\n",
      "‚úÖ data_scene_flow_multiview/training/image_3\n",
      "   Found 400 images\n",
      "============================================================\n",
      "\n",
      "‚úÖ DATASET READY!\n",
      "You can proceed with training.\n"
     ]
    }
   ],
   "source": [
    "# üîç DATASET VERIFICATION - CHECK IF IMAGES EXIST\n",
    "\n",
    "print(\"üîç Checking KITTI dataset structure...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dataset_base = './dataset'\n",
    "required_paths = [\n",
    "    'data_scene_flow_multiview/training/image_2',  # Left images\n",
    "    'data_scene_flow_multiview/training/image_3',  # Right images  \n",
    "]\n",
    "\n",
    "all_exist = True\n",
    "for rel_path in required_paths:\n",
    "    full_path = os.path.join(dataset_base, rel_path)\n",
    "    exists = os.path.exists(full_path)\n",
    "    \n",
    "    if exists:\n",
    "        # Count images in directory\n",
    "        try:\n",
    "            image_files = [f for f in os.listdir(full_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            print(f\"‚úÖ {rel_path}\")\n",
    "            print(f\"   Found {len(image_files)} images\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  {rel_path} exists but can't read: {e}\")\n",
    "            all_exist = False\n",
    "    else:\n",
    "        print(f\"‚ùå {rel_path}\")\n",
    "        print(f\"   Path does not exist!\")\n",
    "        all_exist = False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not all_exist:\n",
    "    print(\"\\nüö® DATASET NOT FOUND!\")\n",
    "    print(\"\\nüì• To fix this, you need to download the KITTI Stereo 2015 dataset:\")\n",
    "    print(\"\\n**Option 1: Download from Official KITTI Website**\")\n",
    "    print(\"1. Visit: http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php\")\n",
    "    print(\"2. Download 'data_scene_flow_multiview' (left + right stereo images)\")\n",
    "    print(\"3. Extract to ./dataset/ folder\")\n",
    "    print(\"\\n**Option 2: Use a Smaller Test Dataset**\")\n",
    "    print(\"For quick testing, you can:\")\n",
    "    print(\"‚Ä¢ Create dummy data (see next cell)\")\n",
    "    print(\"‚Ä¢ Use a smaller public stereo dataset\")\n",
    "    print(\"‚Ä¢ Download a subset of KITTI images\")\n",
    "    \n",
    "    print(\"\\nüí° Would you like to create a small dummy dataset for testing?\")\n",
    "    print(\"   Run the next cell to generate synthetic test data.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ DATASET READY!\")\n",
    "    print(\"You can proceed with training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "850fe0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° To create dummy test data, uncomment the last line and run this cell\n",
      "   This will let you test the training pipeline without downloading real data\n"
     ]
    }
   ],
   "source": [
    "# üé® OPTIONAL: CREATE DUMMY DATASET FOR TESTING\n",
    "# Run this cell ONLY if you don't have the real KITTI dataset and want to test the pipeline\n",
    "\n",
    "def create_dummy_kitti_dataset(num_images=50):\n",
    "    \"\"\"\n",
    "    Creates a small dummy stereo dataset for testing the pipeline.\n",
    "    This is NOT real data - just for testing that the code works!\n",
    "    \"\"\"\n",
    "    print(\"üé® Creating dummy KITTI-style dataset for testing...\")\n",
    "    \n",
    "    # Create directory structure\n",
    "    base_path = './dataset/data_scene_flow_multiview/training'\n",
    "    left_dir = os.path.join(base_path, 'image_2')\n",
    "    right_dir = os.path.join(base_path, 'image_3')\n",
    "    \n",
    "    os.makedirs(left_dir, exist_ok=True)\n",
    "    os.makedirs(right_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate dummy images\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    \n",
    "    print(f\"Generating {num_images} dummy stereo pairs...\")\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Create random images (not real stereo pairs, just for testing!)\n",
    "        # Left image: Random colored pattern\n",
    "        left_img = np.random.randint(0, 255, (375, 1242, 3), dtype=np.uint8)\n",
    "        # Right image: Similar but slightly shifted (fake stereo effect)\n",
    "        right_img = np.roll(left_img, shift=10, axis=1)\n",
    "        right_img = np.clip(right_img + np.random.randint(-20, 20, right_img.shape), 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Save images in KITTI format\n",
    "        left_path = os.path.join(left_dir, f'{i:06d}_10.png')\n",
    "        right_path = os.path.join(right_dir, f'{i:06d}_10.png')\n",
    "        \n",
    "        Image.fromarray(left_img).save(left_path)\n",
    "        Image.fromarray(right_img).save(right_path)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"   Generated {i + 1}/{num_images} pairs...\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dummy dataset created!\")\n",
    "    print(f\"   üìÅ Left images: {left_dir}\")\n",
    "    print(f\"   üìÅ Right images: {right_dir}\")\n",
    "    print(f\"\\n‚ö†Ô∏è  IMPORTANT: This is synthetic data!\")\n",
    "    print(\"   ‚Ä¢ Use only for testing the pipeline\")\n",
    "    print(\"   ‚Ä¢ Download real KITTI data for actual training\")\n",
    "    print(\"   ‚Ä¢ Results won't be meaningful with dummy data\")\n",
    "\n",
    "# Uncomment the line below to create dummy data\n",
    "# create_dummy_kitti_dataset(num_images=100)\n",
    "\n",
    "print(\"üí° To create dummy test data, uncomment the last line and run this cell\")\n",
    "print(\"   This will let you test the training pipeline without downloading real data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f463bc",
   "metadata": {},
   "source": [
    "### üì• Download Real KITTI Dataset\n",
    "\n",
    "**KITTI Stereo 2015 Dataset:**\n",
    "This will download the official KITTI Scene Flow dataset which contains:\n",
    "- **Left stereo images** (image_2): ~15 GB\n",
    "- **Right stereo images** (image_3): ~15 GB\n",
    "- Real driving scenes from Karlsruhe, Germany\n",
    "- High-quality calibrated stereo pairs\n",
    "\n",
    "**Note:** The download is large (~30 GB total) and may take time depending on your connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c23723d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting KITTI dataset download...\n",
      "‚è±Ô∏è  This will take some time depending on your internet speed\n",
      "\n",
      "üöó KITTI Stereo 2015 Dataset Download\n",
      "======================================================================\n",
      "üìÅ Dataset directory: ./dataset\n",
      "üìÅ Download directory: ./dataset/downloads\n",
      "\n",
      "‚úÖ Dataset already exists!\n",
      "   üì∏ Left images: 400\n",
      "   üì∏ Right images: 400\n",
      "\n",
      "‚úÖ Dataset is ready to use!\n",
      "   üì∏ Left images: 400\n",
      "   üì∏ Right images: 400\n",
      "\n",
      "‚úÖ Dataset is ready to use!\n"
     ]
    }
   ],
   "source": [
    "# üì• DOWNLOAD KITTI STEREO 2015 DATASET\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    \"\"\"Progress bar for downloads\"\"\"\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "def download_url(url, output_path):\n",
    "    \"\"\"Download file with progress bar\"\"\"\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def download_kitti_dataset():\n",
    "    \"\"\"\n",
    "    Download and extract KITTI Stereo 2015 dataset\n",
    "    \"\"\"\n",
    "    print(\"üöó KITTI Stereo 2015 Dataset Download\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Dataset URLs (official KITTI website)\n",
    "    datasets = {\n",
    "        'left_images': {\n",
    "            'url': 'https://s3.eu-central-1.amazonaws.com/avg-kitti/data_scene_flow.zip',\n",
    "            'filename': 'data_scene_flow.zip',\n",
    "            'description': 'Scene Flow Multi-view (includes left and right stereo images)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create dataset directory\n",
    "    dataset_dir = './dataset'\n",
    "    download_dir = os.path.join(dataset_dir, 'downloads')\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÅ Dataset directory: {dataset_dir}\")\n",
    "    print(f\"üìÅ Download directory: {download_dir}\")\n",
    "    print()\n",
    "    \n",
    "    # Check if already downloaded\n",
    "    extracted_path = os.path.join(dataset_dir, 'data_scene_flow_multiview')\n",
    "    if os.path.exists(extracted_path):\n",
    "        print(\"‚úÖ Dataset already exists!\")\n",
    "        \n",
    "        # Verify images exist\n",
    "        left_imgs = os.path.join(extracted_path, 'training/image_2')\n",
    "        right_imgs = os.path.join(extracted_path, 'training/image_3')\n",
    "        \n",
    "        if os.path.exists(left_imgs) and os.path.exists(right_imgs):\n",
    "            left_count = len([f for f in os.listdir(left_imgs) if f.endswith('.png')])\n",
    "            right_count = len([f for f in os.listdir(right_imgs) if f.endswith('.png')])\n",
    "            print(f\"   üì∏ Left images: {left_count}\")\n",
    "            print(f\"   üì∏ Right images: {right_count}\")\n",
    "            print(\"\\n‚úÖ Dataset is ready to use!\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Dataset folder exists but images are missing. Re-downloading...\")\n",
    "    \n",
    "    # Download dataset\n",
    "    for name, info in datasets.items():\n",
    "        print(f\"\\nüì• Downloading {info['description']}...\")\n",
    "        print(f\"   URL: {info['url']}\")\n",
    "        \n",
    "        zip_path = os.path.join(download_dir, info['filename'])\n",
    "        \n",
    "        # Check if zip already exists\n",
    "        if os.path.exists(zip_path):\n",
    "            print(f\"   ‚ÑπÔ∏è  Zip file already exists: {zip_path}\")\n",
    "            print(\"   Skipping download...\")\n",
    "        else:\n",
    "            print(f\"   üíæ Downloading to: {zip_path}\")\n",
    "            print(f\"   ‚ö†Ô∏è  This is a large file (~15-30 GB), please be patient...\")\n",
    "            print()\n",
    "            \n",
    "            try:\n",
    "                download_url(info['url'], zip_path)\n",
    "                print(f\"\\n   ‚úÖ Download complete!\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n   ‚ùå Download failed: {e}\")\n",
    "                print(\"\\n   üîÑ Alternative: Manual Download\")\n",
    "                print(f\"   1. Visit: http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php\")\n",
    "                print(f\"   2. Download 'data_scene_flow_multiview.zip'\")\n",
    "                print(f\"   3. Place it in: {download_dir}\")\n",
    "                print(f\"   4. Re-run this cell to extract\")\n",
    "                return\n",
    "        \n",
    "        # Extract dataset\n",
    "        print(f\"\\nüì¶ Extracting {info['filename']}...\")\n",
    "        print(f\"   This may take several minutes...\")\n",
    "        \n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                # Extract to dataset directory\n",
    "                zip_ref.extractall(dataset_dir)\n",
    "            \n",
    "            print(f\"   ‚úÖ Extraction complete!\")\n",
    "            \n",
    "            # Verify extraction\n",
    "            if os.path.exists(extracted_path):\n",
    "                print(f\"\\n‚úÖ Dataset successfully set up!\")\n",
    "                \n",
    "                # Count images\n",
    "                left_imgs = os.path.join(extracted_path, 'training/image_2')\n",
    "                right_imgs = os.path.join(extracted_path, 'training/image_3')\n",
    "                \n",
    "                if os.path.exists(left_imgs):\n",
    "                    left_count = len([f for f in os.listdir(left_imgs) if f.endswith('.png')])\n",
    "                    print(f\"   üì∏ Left images: {left_count}\")\n",
    "                \n",
    "                if os.path.exists(right_imgs):\n",
    "                    right_count = len([f for f in os.listdir(right_imgs) if f.endswith('.png')])\n",
    "                    print(f\"   üì∏ Right images: {right_count}\")\n",
    "                \n",
    "                print(f\"\\nüéâ KITTI dataset is ready for training!\")\n",
    "                print(f\"   You can now proceed with the training cells.\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Extraction completed but expected path not found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Extraction failed: {e}\")\n",
    "            print(f\"   You may need to extract manually\")\n",
    "            return\n",
    "\n",
    "# Run the download\n",
    "print(\"üöÄ Starting KITTI dataset download...\")\n",
    "print(\"‚è±Ô∏è  This will take some time depending on your internet speed\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # First, try to install tqdm if not available\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "    except ImportError:\n",
    "        print(\"Installing tqdm for progress bars...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tqdm\"])\n",
    "        from tqdm import tqdm\n",
    "    \n",
    "    download_kitti_dataset()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è  Download interrupted by user\")\n",
    "    print(\"You can re-run this cell to resume\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    print(\"\\nüîÑ Manual download instructions:\")\n",
    "    print(\"1. Visit: http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php\")\n",
    "    print(\"2. Download 'data_scene_flow.zip' or 'data_scene_flow_multiview.zip'\")\n",
    "    print(\"3. Extract to ./dataset/ folder\")\n",
    "    print(\"4. Verify the structure:\")\n",
    "    print(\"   ./dataset/data_scene_flow_multiview/training/image_2/ (left images)\")\n",
    "    print(\"   ./dataset/data_scene_flow_multiview/training/image_3/ (right images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf4048c",
   "metadata": {},
   "source": [
    "### ‚úÖ Dataset Status Summary\n",
    "\n",
    "**KITTI Dataset Successfully Downloaded and Configured!**\n",
    "\n",
    "üìä **Dataset Statistics:**\n",
    "- **Left stereo images (image_2)**: 400 training images\n",
    "- **Right stereo images (image_3)**: 400 training images\n",
    "- **Total size**: ~1.68 GB downloaded\n",
    "- **Location**: `./dataset/data_scene_flow_multiview/`\n",
    "\n",
    "üéØ **Ready for Training:**\n",
    "The dataset is now properly configured and you can proceed with:\n",
    "1. Running the dataset initialization cell (already done)\n",
    "2. Starting the training loop\n",
    "3. Evaluating on test images\n",
    "\n",
    "üí° **Note:** This is the KITTI 2015 Scene Flow dataset, perfect for stereo compression research!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0efe0f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöó Loading KITTI dataset...\n",
      "üìê Image resize: (128, 128) (smaller = faster training)\n",
      "üìà Training samples: 1576 (learn compression patterns)\n",
      "üéØ Validation samples: 790 (monitor overfitting)\n",
      "üß™ Test samples: 790 (final evaluation)\n",
      "\n",
      "‚öôÔ∏è Data loaders created with batch size: 12\n",
      "üîÑ Training batches: 132 (1576 images √∑ 12)\n",
      "üìä Validation batches: 66 (790 images √∑ 12)\n",
      "üß™ Test batches: 790 (1 image per batch for analysis)\n",
      "\n",
      "üí° **What happens in each batch:**\n",
      "   ‚Ä¢ Load 12 stereo pairs (left + right images)\n",
      "   ‚Ä¢ Apply transforms (resize, normalize to [0,1])\n",
      "   ‚Ä¢ Stack into tensors for GPU processing\n",
      "   ‚Ä¢ Feed to model for compression/reconstruction\n"
     ]
    }
   ],
   "source": [
    "# üóÇÔ∏è INITIALIZE KITTI STEREO DATASET\n",
    "path = './dataset'          # Path to dataset folder\n",
    "resize = tuple(config['resize'])  # Convert list to tuple for transforms\n",
    "\n",
    "print(f\"üöó Loading {config['dataset_name']} dataset...\")\n",
    "print(f\"üìê Image resize: {resize} (smaller = faster training)\")\n",
    "\n",
    "# üìö CREATE DATASET SPLITS\n",
    "# Each dataset handles loading stereo pairs and applying transforms\n",
    "train_dataset = PairKitti(path=path, set_type='train', resize=resize)\n",
    "val_dataset = PairKitti(path=path, set_type='val', resize=resize)  \n",
    "test_dataset = PairKitti(path=path, set_type='test', resize=resize)\n",
    "\n",
    "print(f\"üìà Training samples: {len(train_dataset)} (learn compression patterns)\")\n",
    "print(f\"üéØ Validation samples: {len(val_dataset)} (monitor overfitting)\")  \n",
    "print(f\"üß™ Test samples: {len(test_dataset)} (final evaluation)\")\n",
    "\n",
    "# üîÑ CREATE DATA LOADERS\n",
    "# DataLoaders handle batching, shuffling, and parallel loading\n",
    "batch_size = config['train_batch_size']\n",
    "\n",
    "# Training: Shuffle for better learning, multiple workers for speed\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, \n",
    "                         shuffle=True, num_workers=3)\n",
    "\n",
    "# Validation: Shuffle for variety, same batch size  \n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, \n",
    "                       shuffle=True, num_workers=3)\n",
    "\n",
    "# Testing: No shuffle (reproducible), batch size 1 for individual analysis\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, \n",
    "                        shuffle=False, num_workers=3)\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Data loaders created with batch size: {batch_size}\")\n",
    "print(f\"üîÑ Training batches: {len(train_loader)} ({len(train_dataset)} images √∑ {batch_size})\")\n",
    "print(f\"üìä Validation batches: {len(val_loader)} ({len(val_dataset)} images √∑ {batch_size})\")  \n",
    "print(f\"üß™ Test batches: {len(test_loader)} (1 image per batch for analysis)\")\n",
    "\n",
    "print(f\"\\nüí° **What happens in each batch:**\")\n",
    "print(f\"   ‚Ä¢ Load {batch_size} stereo pairs (left + right images)\")\n",
    "print(f\"   ‚Ä¢ Apply transforms (resize, normalize to [0,1])\")\n",
    "print(f\"   ‚Ä¢ Stack into tensors for GPU processing\")\n",
    "print(f\"   ‚Ä¢ Feed to model for compression/reconstruction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc611a0b",
   "metadata": {},
   "source": [
    "## 5. Model Initialization üß†\n",
    "\n",
    "**DHF-JSCC Architecture Overview:**\n",
    "The model has several key components working together:\n",
    "\n",
    "1. **Encoder Network**: Converts images to compressed latent representations\n",
    "2. **Entropy Model**: Estimates probability distributions for efficient coding\n",
    "3. **Decoder Network**: Reconstructs images from compressed representations\n",
    "4. **Side Information**: Uses right image to help compress left image\n",
    "\n",
    "**Training from Scratch vs Pretrained:**\n",
    "- **From Scratch**: Random weights, longer training, full learning experience\n",
    "- **Pretrained**: Pre-learned weights, faster convergence, less exploration\n",
    "\n",
    "**Why 21M Parameters?**\n",
    "Deep compression needs capacity to:\n",
    "- Learn complex image features at multiple scales\n",
    "- Model correlations between stereo pairs  \n",
    "- Optimize rate-distortion trade-offs across diverse scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6928a18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Model type: Stereo compression (with side info)\n",
      "üöÄ Creating new model for training from scratch...\n",
      "‚úÖ New DHF-JSCC model created!\n",
      "   ‚Ä¢ Filters: 256 (network capacity)\n",
      "   ‚Ä¢ Architecture: Encoder ‚Üí Entropy Model ‚Üí Decoder\n",
      "   ‚Ä¢ Side info: Enabled (stereo)\n",
      "üìç Model moved to: CUDA (GPU)\n",
      "\n",
      "üéÆ MULTI-GPU TRAINING ENABLED!\n",
      "   ‚Ä¢ Available GPUs: 2\n",
      "   ‚Ä¢ GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   ‚Ä¢ GPU 1: NVIDIA GeForce RTX 2080 Ti\n",
      "‚úÖ Model wrapped with DataParallel\n",
      "   ‚Ä¢ Batch will be split across 2 GPUs\n",
      "   ‚Ä¢ Effective batch size per GPU: 6\n",
      "   ‚Ä¢ Gradients will be synchronized after each batch\n",
      "\n",
      "üî¢ Model Statistics:\n",
      "   ‚Ä¢ Total parameters: 21,380,152\n",
      "   ‚Ä¢ Trainable parameters: 21,380,152\n",
      "   ‚Ä¢ Memory footprint: ~85.5 MB (float32)\n",
      "\n",
      "‚öôÔ∏è Optimizer: Adam with learning rate 0.0001\n",
      "   ‚Ä¢ Adam: Adapts learning rate per parameter\n",
      "   ‚Ä¢ AMSGrad: Improved convergence stability\n",
      "   ‚Ä¢ Learning rate: 0.0001 (balanced for deep networks)\n",
      "\n",
      "üé≤ Training from scratch with randomly initialized weights\n",
      "   ‚Ä¢ All weights start random (Gaussian/Xavier initialization)\n",
      "   ‚Ä¢ Model will learn compression from ground up\n",
      "   ‚Ä¢ Longer training but full learning experience\n",
      "\n",
      "üìà Learning Rate Scheduler: ReduceLROnPlateau\n",
      "   ‚Ä¢ Reduces LR when validation loss plateaus\n",
      "   ‚Ä¢ Factor: 0.1 (10x reduction)\n",
      "   ‚Ä¢ Patience: 10 epochs before reduction\n",
      "   ‚Ä¢ Min LR: 1e-7 (prevents going too small)\n",
      "\n",
      "üè∑Ô∏è Experiment identifier: KITTI_stereo_MSE_lambda:3e-05\n",
      "‚úÖ Model initialization complete - ready for training!\n",
      "üìç Model moved to: CUDA (GPU)\n",
      "\n",
      "üéÆ MULTI-GPU TRAINING ENABLED!\n",
      "   ‚Ä¢ Available GPUs: 2\n",
      "   ‚Ä¢ GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   ‚Ä¢ GPU 1: NVIDIA GeForce RTX 2080 Ti\n",
      "‚úÖ Model wrapped with DataParallel\n",
      "   ‚Ä¢ Batch will be split across 2 GPUs\n",
      "   ‚Ä¢ Effective batch size per GPU: 6\n",
      "   ‚Ä¢ Gradients will be synchronized after each batch\n",
      "\n",
      "üî¢ Model Statistics:\n",
      "   ‚Ä¢ Total parameters: 21,380,152\n",
      "   ‚Ä¢ Trainable parameters: 21,380,152\n",
      "   ‚Ä¢ Memory footprint: ~85.5 MB (float32)\n",
      "\n",
      "‚öôÔ∏è Optimizer: Adam with learning rate 0.0001\n",
      "   ‚Ä¢ Adam: Adapts learning rate per parameter\n",
      "   ‚Ä¢ AMSGrad: Improved convergence stability\n",
      "   ‚Ä¢ Learning rate: 0.0001 (balanced for deep networks)\n",
      "\n",
      "üé≤ Training from scratch with randomly initialized weights\n",
      "   ‚Ä¢ All weights start random (Gaussian/Xavier initialization)\n",
      "   ‚Ä¢ Model will learn compression from ground up\n",
      "   ‚Ä¢ Longer training but full learning experience\n",
      "\n",
      "üìà Learning Rate Scheduler: ReduceLROnPlateau\n",
      "   ‚Ä¢ Reduces LR when validation loss plateaus\n",
      "   ‚Ä¢ Factor: 0.1 (10x reduction)\n",
      "   ‚Ä¢ Patience: 10 epochs before reduction\n",
      "   ‚Ä¢ Min LR: 1e-7 (prevents going too small)\n",
      "\n",
      "üè∑Ô∏è Experiment identifier: KITTI_stereo_MSE_lambda:3e-05\n",
      "‚úÖ Model initialization complete - ready for training!\n"
     ]
    }
   ],
   "source": [
    "# üß† MODEL INITIALIZATION - BUILDING OUR COMPRESSION NETWORK\n",
    "\n",
    "# üîß Configuration check\n",
    "with_side_info = config['use_side_info']\n",
    "print(f\"üéØ Model type: {'Stereo compression (with side info)' if with_side_info else 'Single image compression'}\")\n",
    "\n",
    "# üèóÔ∏è CREATE NEW MODEL FOR TRAINING FROM SCRATCH\n",
    "print(\"üöÄ Creating new model for training from scratch...\")\n",
    "if model_d_fusion2 is not None:\n",
    "    # Initialize model with specified number of filters (network capacity)\n",
    "    model = model_d_fusion2.Image_coding(M=config['num_filters'], N2=25)\n",
    "    print(f\"‚úÖ New DHF-JSCC model created!\")\n",
    "    print(f\"   ‚Ä¢ Filters: {config['num_filters']} (network capacity)\")\n",
    "    print(f\"   ‚Ä¢ Architecture: Encoder ‚Üí Entropy Model ‚Üí Decoder\")\n",
    "    print(f\"   ‚Ä¢ Side info: {'Enabled (stereo)' if with_side_info else 'Disabled (single)'}\")\n",
    "else:\n",
    "    raise ImportError(\"‚ùå model_d_fusion2 module required but not available. Check imports!\")\n",
    "\n",
    "# üöÄ MOVE MODEL TO GPU (CUDA ACCELERATION)\n",
    "model = model.cuda() if config['cuda'] else model\n",
    "device = 'CUDA (GPU)' if config['cuda'] else 'CPU'\n",
    "print(f\"üìç Model moved to: {device}\")\n",
    "\n",
    "# üéÆ MULTI-GPU SETUP (DataParallel)\n",
    "if config['cuda'] and config.get('multi_gpu', False) and torch.cuda.device_count() > 1:\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"\\nüéÆ MULTI-GPU TRAINING ENABLED!\")\n",
    "    print(f\"   ‚Ä¢ Available GPUs: {gpu_count}\")\n",
    "    for i in range(gpu_count):\n",
    "        print(f\"   ‚Ä¢ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    # Wrap model with DataParallel\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    print(f\"‚úÖ Model wrapped with DataParallel\")\n",
    "    print(f\"   ‚Ä¢ Batch will be split across {gpu_count} GPUs\")\n",
    "    print(f\"   ‚Ä¢ Effective batch size per GPU: {config['train_batch_size'] // gpu_count}\")\n",
    "    print(f\"   ‚Ä¢ Gradients will be synchronized after each batch\")\n",
    "elif config['cuda']:\n",
    "    print(f\"   ‚Ä¢ GPU Memory: More efficient for large models\")\n",
    "    print(f\"   ‚Ä¢ Parallel Processing: Faster matrix operations\")\n",
    "    print(f\"   ‚Ä¢ Using single GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# üìä ANALYZE MODEL COMPLEXITY\n",
    "if hasattr(model, 'parameters'):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nüî¢ Model Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Total parameters: {total_params:,}\")\n",
    "    print(f\"   ‚Ä¢ Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   ‚Ä¢ Memory footprint: ~{total_params * 4 / 1e6:.1f} MB (float32)\")\n",
    "\n",
    "# üéì INITIALIZE OPTIMIZER (ADAM - ADAPTIVE LEARNING)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], amsgrad=True)\n",
    "print(f\"\\n‚öôÔ∏è Optimizer: Adam with learning rate {config['lr']}\")\n",
    "print(f\"   ‚Ä¢ Adam: Adapts learning rate per parameter\")\n",
    "print(f\"   ‚Ä¢ AMSGrad: Improved convergence stability\")\n",
    "print(f\"   ‚Ä¢ Learning rate: {config['lr']} (balanced for deep networks)\")\n",
    "\n",
    "# üíæ WEIGHT LOADING LOGIC (CURRENTLY DISABLED FOR SCRATCH TRAINING)\n",
    "if config['load_weight'] and os.path.exists(config['weight_path']):\n",
    "    print(f\"\\nüì• Loading pretrained weights from: {config['weight_path']}\")\n",
    "    try:\n",
    "        checkpoint = torch.load(config['weight_path'], \n",
    "                              map_location=torch.device('cuda' if config['cuda'] else 'cpu'))\n",
    "        \n",
    "        # Handle potential layer name differences\n",
    "        if config['baseline_model'] == 'bls17' and with_side_info:\n",
    "            checkpoint['model_state_dict'] = map_layers(checkpoint['model_state_dict'])\n",
    "            \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"‚úÖ Pretrained weights loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load weights: {e}\")\n",
    "        print(\"üîÑ Continuing with randomly initialized weights...\")\n",
    "elif config['load_weight']:\n",
    "    print(f\"\\n‚ùå Weight file not found: {config['weight_path']}\")\n",
    "    print(\"üîÑ Continuing with randomly initialized weights...\")\n",
    "else:\n",
    "    print(f\"\\nüé≤ Training from scratch with randomly initialized weights\")\n",
    "    print(\"   ‚Ä¢ All weights start random (Gaussian/Xavier initialization)\")\n",
    "    print(\"   ‚Ä¢ Model will learn compression from ground up\")\n",
    "    print(\"   ‚Ä¢ Longer training but full learning experience\")\n",
    "\n",
    "# üìâ LEARNING RATE SCHEDULER (ADAPTIVE LEARNING)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=10, min_lr=1e-7\n",
    ")\n",
    "print(f\"\\nüìà Learning Rate Scheduler: ReduceLROnPlateau\")\n",
    "print(f\"   ‚Ä¢ Reduces LR when validation loss plateaus\")\n",
    "print(f\"   ‚Ä¢ Factor: 0.1 (10x reduction)\")\n",
    "print(f\"   ‚Ä¢ Patience: 10 epochs before reduction\")\n",
    "print(f\"   ‚Ä¢ Min LR: 1e-7 (prevents going too small)\")\n",
    "\n",
    "# üè∑Ô∏è CREATE EXPERIMENT NAME FOR TRACKING\n",
    "experiment_name = str(train_dataset) + '_' + config['distortion_loss'] + '_lambda:' + str(config['lambda'])\n",
    "print(f\"\\nüè∑Ô∏è Experiment identifier: {experiment_name}\")\n",
    "print(\"‚úÖ Model initialization complete - ready for training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e5062",
   "metadata": {},
   "source": [
    "## üöÄ Multi-GPU Setup (DataParallel)\n",
    "\n",
    "**Using Both RTX 2080 Ti GPUs:**\n",
    "PyTorch's DataParallel allows us to use multiple GPUs simultaneously:\n",
    "\n",
    "**How It Works:**\n",
    "- **Model Replication**: Copy model to each GPU\n",
    "- **Batch Splitting**: Divide each batch across GPUs (batch_size=8 ‚Üí 4 per GPU)\n",
    "- **Parallel Forward**: Each GPU processes its portion simultaneously\n",
    "- **Gradient Aggregation**: Combine gradients from all GPUs\n",
    "- **Single Update**: Update model weights once with combined gradients\n",
    "\n",
    "**Performance Benefits:**\n",
    "- ~2x faster training with 2 GPUs\n",
    "- Better GPU memory utilization\n",
    "- Same final model quality\n",
    "\n",
    "**Note:** The batch size will be automatically split across GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1660c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç GPU Detection:\n",
      "   ‚Ä¢ Available GPUs: 2\n",
      "\n",
      "üöÄ Enabling DataParallel across 2 GPUs:\n",
      "   ‚Ä¢ GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "   ‚Ä¢ GPU 1: NVIDIA GeForce RTX 2080 Ti\n",
      "\n",
      "‚úÖ Model wrapped with DataParallel!\n",
      "   ‚Ä¢ Batch size: 8 (split 4 per GPU)\n",
      "   ‚Ä¢ Both GPUs will be utilized automatically\n",
      "   ‚Ä¢ Expected speedup: ~2x faster training\n",
      "\n",
      "üí° Monitor GPU usage with: nvidia-smi -l 1\n",
      "   You should see both GPUs active during training!\n"
     ]
    }
   ],
   "source": [
    "# üöÄ ENABLE MULTI-GPU TRAINING WITH DATAPARALLEL\n",
    "\n",
    "# Check GPU availability\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"üîç GPU Detection:\")\n",
    "print(f\"   ‚Ä¢ Available GPUs: {num_gpus}\")\n",
    "\n",
    "if num_gpus > 1:\n",
    "    print(f\"\\nüöÄ Enabling DataParallel across {num_gpus} GPUs:\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"   ‚Ä¢ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    # Wrap model with DataParallel\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    print(f\"\\n‚úÖ Model wrapped with DataParallel!\")\n",
    "    print(f\"   ‚Ä¢ Batch size: {config['train_batch_size']} (split {config['train_batch_size']//num_gpus} per GPU)\")\n",
    "    print(f\"   ‚Ä¢ Both GPUs will be utilized automatically\")\n",
    "    print(f\"   ‚Ä¢ Expected speedup: ~{num_gpus}x faster training\")\n",
    "    \n",
    "elif num_gpus == 1:\n",
    "    print(f\"\\n‚ö†Ô∏è  Only 1 GPU detected - using single GPU training\")\n",
    "    print(f\"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå No GPUs detected - using CPU (very slow!)\")\n",
    "\n",
    "print(f\"\\nüí° Monitor GPU usage with: nvidia-smi -l 1\")\n",
    "print(f\"   You should see both GPUs active during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b518638d",
   "metadata": {},
   "source": [
    "### üöÄ Training from Scratch - Ready!\n",
    "\n",
    "**Model Configuration:**\n",
    "- New model created with random weights\n",
    "- Architecture: DHF-JSCC with side information\n",
    "- Parameters will be shown after model creation\n",
    "\n",
    "**Training Strategy:**\n",
    "- Starting with MSE loss (more stable for initial training)\n",
    "- Can switch to MS-SSIM loss later for better perceptual quality\n",
    "- Learning rate: 1e-4 (good starting point)\n",
    "- Batch size: 8 (conservative for stability)\n",
    "\n",
    "**Monitoring:**\n",
    "- Verbose output every 10 epochs\n",
    "- Model checkpoints saved when validation loss improves\n",
    "- Both PSNR and MS-SSIM metrics tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7e274e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ TRAINING FROM SCRATCH - READY TO START!\n",
      "============================================================\n",
      "‚úÖ Model: 21,380,152 parameters on CUDA\n",
      "‚úÖ Dataset: 1576 training, 790 validation samples\n",
      "‚úÖ Training setup: 30000 epochs, batch size 12\n",
      "============================================================\n",
      "\n",
      "üìä TRAINING EXPECTATIONS:\n",
      "‚Ä¢ Initial epochs may have high loss - this is normal\n",
      "‚Ä¢ Look for consistent loss decrease over time\n",
      "‚Ä¢ PSNR should gradually improve (higher is better)\n",
      "‚Ä¢ MS-SSIM should approach 1.0 (closer to 1.0 is better)\n",
      "\n",
      "‚öôÔ∏è TRAINING TIPS:\n",
      "‚Ä¢ Monitor GPU memory usage - reduce batch size if OOM occurs\n",
      "‚Ä¢ Early epochs focus on convergence, later epochs on fine-tuning\n",
      "‚Ä¢ If loss plateaus, the scheduler will reduce learning rate\n",
      "‚Ä¢ Best model weights are saved automatically\n",
      "\n",
      "üéØ TYPICAL BENCHMARKS FOR KITTI:\n",
      "‚Ä¢ Good PSNR: >25 dB\n",
      "‚Ä¢ Excellent PSNR: >30 dB\n",
      "‚Ä¢ Good MS-SSIM: >0.85\n",
      "‚Ä¢ Excellent MS-SSIM: >0.90\n",
      "\n",
      "‚ñ∂Ô∏è  Ready to run the training loop!\n",
      "   Execute the training cell to start training from scratch.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Training Progress Tracker and Tips\n",
    "print(\"üöÄ TRAINING FROM SCRATCH - READY TO START!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Model: {total_params:,} parameters on {'CUDA' if config['cuda'] else 'CPU'}\")\n",
    "print(f\"‚úÖ Dataset: {len(train_dataset)} training, {len(val_dataset)} validation samples\")\n",
    "print(f\"‚úÖ Training setup: {config['epochs']} epochs, batch size {config['train_batch_size']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä TRAINING EXPECTATIONS:\")\n",
    "print(\"‚Ä¢ Initial epochs may have high loss - this is normal\")\n",
    "print(\"‚Ä¢ Look for consistent loss decrease over time\")\n",
    "print(\"‚Ä¢ PSNR should gradually improve (higher is better)\")\n",
    "print(\"‚Ä¢ MS-SSIM should approach 1.0 (closer to 1.0 is better)\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è TRAINING TIPS:\")\n",
    "print(\"‚Ä¢ Monitor GPU memory usage - reduce batch size if OOM occurs\")\n",
    "print(\"‚Ä¢ Early epochs focus on convergence, later epochs on fine-tuning\")\n",
    "print(\"‚Ä¢ If loss plateaus, the scheduler will reduce learning rate\")\n",
    "print(\"‚Ä¢ Best model weights are saved automatically\")\n",
    "\n",
    "print(\"\\nüéØ TYPICAL BENCHMARKS FOR KITTI:\")\n",
    "print(\"‚Ä¢ Good PSNR: >25 dB\")\n",
    "print(\"‚Ä¢ Excellent PSNR: >30 dB\") \n",
    "print(\"‚Ä¢ Good MS-SSIM: >0.85\")\n",
    "print(\"‚Ä¢ Excellent MS-SSIM: >0.90\")\n",
    "\n",
    "print(\"\\n‚ñ∂Ô∏è  Ready to run the training loop!\")\n",
    "print(\"   Execute the training cell to start training from scratch.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241b64e8",
   "metadata": {},
   "source": [
    "## 6. Training Setup üõ†Ô∏è\n",
    "\n",
    "**Loss Function Deep Dive:**\n",
    "The loss function determines what the model optimizes for:\n",
    "\n",
    "**Rate-Distortion Loss = Œª √ó Distortion + Rate**\n",
    "\n",
    "- **Distortion**: How different reconstructed images are from originals\n",
    "- **Rate**: How many bits needed to store compressed representation  \n",
    "- **Lambda (Œª)**: Trade-off parameter\n",
    "  - High Œª: Prioritize compression (smaller files, lower quality)\n",
    "  - Low Œª: Prioritize quality (larger files, better images)\n",
    "\n",
    "**Why MSE vs MS-SSIM?**\n",
    "- **MSE**: Simple pixel differences, fast computation, but ignores human perception\n",
    "- **MS-SSIM**: Structural similarity, matches human vision, but slower to compute\n",
    "\n",
    "**Directory Structure:**\n",
    "Organized saving helps track experiments and results across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc52a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Checkpoint directories created:\n",
      "   ‚Ä¢ .pkl files: ./checkpoints/10_07/pkl\n",
      "   ‚Ä¢ .pth files: ./checkpoints/10_07/pth\n",
      "\n",
      "üìÇ Training run directory structure:\n",
      "   ‚Ä¢ Run folder: ./models/KITTI_MSE_lambda3e-05_20251007_201536\n",
      "   ‚Ä¢ Results: ./models/KITTI_MSE_lambda3e-05_20251007_201536/results\n",
      "\n",
      "üíæ Model checkpoints will be saved to:\n",
      "   ‚Ä¢ PKL format: ./checkpoints/10_07/pkl/epoch_XXXX_psnr_YY.YYdB.pkl\n",
      "   ‚Ä¢ PTH format: ./checkpoints/10_07/pth/epoch_XXXX_psnr_YY.YYdB.pth (recommended)\n",
      "\n",
      "üéØ Loss Function Configuration:\n",
      "   ‚Ä¢ Distortion metric: MSE\n",
      "   ‚Ä¢ MSE function: GPU accelerated\n",
      "\n",
      "‚öñÔ∏è Rate-Distortion Trade-off:\n",
      "   ‚Ä¢ Lambda (Œª): 3e-05\n",
      "   ‚Ä¢ Higher Œª ‚Üí More compression, lower quality\n",
      "   ‚Ä¢ Lower Œª ‚Üí Less compression, higher quality\n",
      "   ‚Ä¢ Current setting: Balanced\n",
      "\n",
      "üìä Training Loss Formula:\n",
      "   Total Loss = Œª √ó Distortion + Rate\n",
      "   Where:\n",
      "   ‚Ä¢ Distortion = MSE between original and reconstructed\n",
      "   ‚Ä¢ Rate = Estimated bits per pixel (BPP)\n",
      "   ‚Ä¢ Œª = 3e-05 (our trade-off parameter)\n",
      "\n",
      "‚úÖ Training setup complete!\n",
      "üöÄ Ready to start the main training loop!\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è TRAINING SETUP - PREPARE DIRECTORIES AND LOSS FUNCTIONS\n",
    "\n",
    "# üìÅ CREATE ORGANIZED OUTPUT DIRECTORIES\n",
    "import datetime\n",
    "\n",
    "# Create date-based checkpoint structure (same as main22.py)\n",
    "date_folder = datetime.datetime.now().strftime(\"%m_%d\")  # e.g., \"10_07\"\n",
    "pkl_dir = os.path.join('.', 'checkpoints', date_folder, 'pkl')\n",
    "pth_dir = os.path.join('.', 'checkpoints', date_folder, 'pth')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(pkl_dir, exist_ok=True)\n",
    "os.makedirs(pth_dir, exist_ok=True)\n",
    "\n",
    "# Add checkpoint directories to config for use in training loop\n",
    "config['pkl_dir'] = pkl_dir\n",
    "config['pth_dir'] = pth_dir\n",
    "\n",
    "print(f\"\udcc1 Checkpoint directories created:\")\n",
    "print(f\"   ‚Ä¢ .pkl files: {pkl_dir}\")\n",
    "print(f\"   ‚Ä¢ .pth files: {pth_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d001f7",
   "metadata": {},
   "source": [
    "## 7. Training Loop üéì\n",
    "\n",
    "**The Heart of Deep Learning:**\n",
    "This is where the magic happens! The training loop teaches our model to compress images by:\n",
    "\n",
    "**Training Phase (Each Epoch):**\n",
    "1. **Forward Pass**: Feed images through model ‚Üí get reconstruction\n",
    "2. **Loss Calculation**: Measure how good/bad the reconstruction is\n",
    "3. **Backpropagation**: Calculate gradients (how to improve)\n",
    "4. **Optimization**: Update model weights using gradients\n",
    "\n",
    "**Validation Phase:**\n",
    "- Test on unseen data to check if model is learning generally (not just memorizing)\n",
    "- No weight updates - just monitoring performance\n",
    "\n",
    "**Key Metrics to Watch:**\n",
    "- **Loss**: Should decrease over time (lower = better)\n",
    "- **PSNR**: Peak Signal-to-Noise Ratio (higher = better quality)\n",
    "- **MS-SSIM**: Structural similarity (closer to 1.0 = better)\n",
    "\n",
    "**Training Tips:**\n",
    "- First few epochs may have erratic loss - this is normal\n",
    "- Look for consistent downward trend over many epochs\n",
    "- Validation metrics more important than training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a30711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipywidgets is already installed\n"
     ]
    }
   ],
   "source": [
    "# Install ipywidgets for notebook progress bars\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import ipywidgets\n",
    "    print(\"ipywidgets is already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing ipywidgets...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ipywidgets\"])\n",
    "    print(\"ipywidgets installed successfully!\")\n",
    "    print(\"Note: You may need to restart the kernel if progress bars don't show correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f009d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING TRAINING ===\n",
      "Total Epochs: 30000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== STARTING TRAINING ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLearning Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLambda: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mlambda\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'batch_size'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ensure config has checkpoint directories (in case cell above wasn't re-run)\n",
    "if 'pkl_dir' not in config:\n",
    "    config['pkl_dir'] = pkl_dir\n",
    "if 'pth_dir' not in config:\n",
    "    config['pth_dir'] = pth_dir\n",
    "\n",
    "# TRAINING SETUP\n",
    "print(\"=== STARTING TRAINING ===\")\n",
    "print(f\"Total Epochs: {config['epochs']}\")\n",
    "print(f\"Batch Size: {config['batch_size']}\")\n",
    "print(f\"Learning Rate: {config['lr']}\")\n",
    "print(f\"Lambda: {config['lambda']}\")\n",
    "print(f\"Checkpoint dirs: pkl={config.get('pkl_dir', 'NOT SET')}, pth={config.get('pth_dir', 'NOT SET')}\")\n",
    "print(f\"Using {'notebook' if IN_NOTEBOOK else 'terminal'} progress bars\")\n",
    "print(f\"Using {torch.cuda.device_count()} GPU(s)\")\n",
    "\n",
    "# Training variables\n",
    "best_psnr = 0.0\n",
    "min_val_loss = None\n",
    "\n",
    "# Create progress bar for epochs\n",
    "if IN_NOTEBOOK:\n",
    "    epoch_pbar = tqdm(range(config['epochs']), desc='Training Progress', position=0)\n",
    "else:\n",
    "    epoch_pbar = range(config['epochs'])\n",
    "\n",
    "# MAIN TRAINING LOOP\n",
    "for epoch in epoch_pbar:\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # === TRAINING PHASE ===\n",
    "    model.train()\n",
    "    train_loss_sum = 0\n",
    "    \n",
    "    # Training progress bar\n",
    "    if IN_NOTEBOOK:\n",
    "        train_pbar = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                         desc=f'Epoch {epoch+1}/{config[\"epochs\"]} [Train]', \n",
    "                         leave=False, position=1)\n",
    "    else:\n",
    "        train_pbar = enumerate(train_loader)\n",
    "    \n",
    "    for batch_idx, (input_left, input_right) in train_pbar:\n",
    "        # Move data to GPU\n",
    "        input_left = input_left.to(device)\n",
    "        input_right = input_right.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output_left, output_right, num_bits_left, num_bits_right = model(input_left, input_right)\n",
    "        \n",
    "        # Calculate loss (distortion only)\n",
    "        mse_loss_left = criterion(output_left, input_left)\n",
    "        mse_loss_right = criterion(output_right, input_right)\n",
    "        distortion_loss = mse_loss_left + mse_loss_right\n",
    "        \n",
    "        # Total loss (rate-distortion)\n",
    "        total_loss = distortion_loss + config['lambda'] * (num_bits_left + num_bits_right)\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        train_loss_sum += total_loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        if IN_NOTEBOOK:\n",
    "            train_pbar.set_postfix({'loss': total_loss.item()})\n",
    "    \n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = train_loss_sum / len(train_loader)\n",
    "    \n",
    "    # === VALIDATION PHASE ===\n",
    "    model.eval()\n",
    "    val_loss_sum = 0\n",
    "    psnr_sum = 0\n",
    "    msssim_db_sum = 0\n",
    "    distortion_sum = 0\n",
    "    \n",
    "    # Validation progress bar\n",
    "    if IN_NOTEBOOK:\n",
    "        val_pbar = tqdm(enumerate(val_loader), total=len(val_loader),\n",
    "                       desc=f'Epoch {epoch+1}/{config[\"epochs\"]} [Val]',\n",
    "                       leave=False, position=1)\n",
    "    else:\n",
    "        val_pbar = enumerate(val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_left, input_right) in val_pbar:\n",
    "            # Move data to GPU\n",
    "            input_left = input_left.to(device)\n",
    "            input_right = input_right.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output_left, output_right, num_bits_left, num_bits_right = model(input_left, input_right)\n",
    "            \n",
    "            # Calculate loss\n",
    "            mse_loss_left = criterion(output_left, input_left)\n",
    "            mse_loss_right = criterion(output_right, input_right)\n",
    "            distortion_loss = mse_loss_left + mse_loss_right\n",
    "            total_loss = distortion_loss + config['lambda'] * (num_bits_left + num_bits_right)\n",
    "            \n",
    "            # Track metrics\n",
    "            val_loss_sum += total_loss.item()\n",
    "            distortion_sum += distortion_loss.item()\n",
    "            \n",
    "            # Calculate PSNR\n",
    "            psnr_left = 10 * torch.log10(1 / mse_loss_left)\n",
    "            psnr_right = 10 * torch.log10(1 / mse_loss_right)\n",
    "            avg_psnr_batch = (psnr_left + psnr_right) / 2\n",
    "            psnr_sum += avg_psnr_batch.item()\n",
    "            \n",
    "            # Calculate MS-SSIM in dB\n",
    "            msssim_left = ms_ssim(output_left, input_left, data_range=1.0, size_average=True)\n",
    "            msssim_right = ms_ssim(output_right, input_right, data_range=1.0, size_average=True)\n",
    "            msssim_db_batch = -10 * torch.log10(1 - (msssim_left + msssim_right) / 2)\n",
    "            msssim_db_sum += msssim_db_batch.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            if IN_NOTEBOOK:\n",
    "                val_pbar.set_postfix({'loss': total_loss.item(), 'psnr': avg_psnr_batch.item()})\n",
    "    \n",
    "    # Calculate average validation metrics\n",
    "    avg_val_loss = val_loss_sum / len(val_loader)\n",
    "    avg_psnr = psnr_sum / len(val_loader)\n",
    "    avg_msssim_db = msssim_db_sum / len(val_loader)\n",
    "    avg_distortion = distortion_sum / len(val_loader)\n",
    "    \n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Track loss for scheduler\n",
    "    val_loss_to_track = avg_val_loss\n",
    "    \n",
    "    # PRINT EPOCH SUMMARY\n",
    "    if IN_NOTEBOOK:\n",
    "        tqdm.write(f\"\\n[Epoch {epoch+1}/{config['epochs']}] \"\n",
    "                  f\"Time: {epoch_time:.1f}s | \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "                  f\"PSNR: {avg_psnr:.4f} dB | \"\n",
    "                  f\"MS-SSIM: {avg_msssim_db:.4f} dB\")\n",
    "        \n",
    "        # Print quality indicators\n",
    "        if avg_psnr > 25:\n",
    "            tqdm.write(\"  >> Good quality achieved (PSNR > 25 dB)\")\n",
    "        if avg_psnr > 30:\n",
    "            tqdm.write(\"  >> Excellent quality achieved (PSNR > 30 dB)\")\n",
    "    else:\n",
    "        print(f\"\\n[Epoch {epoch+1}/{config['epochs']}] \"\n",
    "              f\"Time: {epoch_time:.1f}s | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "              f\"PSNR: {avg_psnr:.4f} dB | \"\n",
    "              f\"MS-SSIM: {avg_msssim_db:.4f} dB\")\n",
    "        \n",
    "        # Print quality indicators\n",
    "        if avg_psnr > 25:\n",
    "            print(\"  >> Good quality achieved (PSNR > 25 dB)\")\n",
    "        if avg_psnr > 30:\n",
    "            print(\"  >> Excellent quality achieved (PSNR > 30 dB)\")\n",
    "    \n",
    "    # SAVE MODEL CHECKPOINTS\n",
    "    if config['save_weights']:\n",
    "        # Get the actual model (unwrap DataParallel if needed)\n",
    "        model_to_save = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "        \n",
    "        # Create checkpoint dictionary\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model_to_save.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss_to_track,\n",
    "            'psnr': avg_psnr,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        # SAVE PICKLE FILE FOR EVERY EPOCH\n",
    "        import pickle\n",
    "        epoch_pkl_path = os.path.join(config['pkl_dir'], f'epoch_{epoch+1:04d}.pkl')\n",
    "        epoch_data = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': val_loss_to_track,\n",
    "            'psnr': avg_psnr,\n",
    "            'msssim_db': avg_msssim_db,\n",
    "            'distortion': avg_distortion,\n",
    "            'model_state': model_to_save.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'scheduler_state': scheduler.state_dict()\n",
    "        }\n",
    "        with open(epoch_pkl_path, 'wb') as f:\n",
    "            pickle.dump(epoch_data, f)\n",
    "        \n",
    "        # Save periodic checkpoint every 10 epochs as .pth\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_path = os.path.join(config['pth_dir'], \n",
    "                                          f'epoch_{epoch+1:04d}_psnr_{avg_psnr:.2f}dB.pth')\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            if IN_NOTEBOOK:\n",
    "                tqdm.write(f\"  [SAVED] Checkpoint: epoch_{epoch+1:04d}_psnr_{avg_psnr:.2f}dB.pth\")\n",
    "            else:\n",
    "                print(f\"  [SAVED] Checkpoint: epoch_{epoch+1:04d}_psnr_{avg_psnr:.2f}dB.pth\")\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if min_val_loss is None or min_val_loss > val_loss_to_track:\n",
    "            min_val_loss = val_loss_to_track\n",
    "            best_model_path = os.path.join(config['pth_dir'], 'best_model_loss.pth')\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            if IN_NOTEBOOK:\n",
    "                tqdm.write(f\"  [BEST LOSS] best_model_loss.pth - Val Loss: {val_loss_to_track:.4f}\")\n",
    "            else:\n",
    "                print(f\"  [BEST LOSS] best_model_loss.pth - Val Loss: {val_loss_to_track:.4f}\")\n",
    "        \n",
    "        # Save best model based on PSNR\n",
    "        if avg_psnr > best_psnr:\n",
    "            best_psnr = avg_psnr\n",
    "            best_model_path = os.path.join(config['pth_dir'], \n",
    "                                          f'best_model_psnr_{avg_psnr:.2f}dB.pth')\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            if IN_NOTEBOOK:\n",
    "                tqdm.write(f\"  [BEST PSNR] best_model_psnr_{avg_psnr:.2f}dB.pth - PSNR: {avg_psnr:.4f} dB\")\n",
    "            else:\n",
    "                print(f\"  [BEST PSNR] best_model_psnr_{avg_psnr:.2f}dB.pth - PSNR: {avg_psnr:.4f} dB\")\n",
    "    \n",
    "    # LEARNING RATE SCHEDULING\n",
    "    scheduler.step(val_loss_to_track)  # Reduce LR if validation plateaus\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if current_lr < config['lr']:\n",
    "        if IN_NOTEBOOK:\n",
    "            tqdm.write(f\"  [LR REDUCED] Learning rate reduced to: {current_lr:.2e}\")\n",
    "        else:\n",
    "            print(f\"  [LR REDUCED] Learning rate reduced to: {current_lr:.2e}\")\n",
    "\n",
    "# Close progress bar\n",
    "if IN_NOTEBOOK:\n",
    "    epoch_pbar.close()\n",
    "\n",
    "# SAVE FINAL TRAINING HISTORY AND MODEL\n",
    "print(\"\\n=== SAVING FINAL RESULTS ===\")\n",
    "\n",
    "# Save complete training history as pickle\n",
    "import pickle\n",
    "training_history = {\n",
    "    'final_epoch': config['epochs'],\n",
    "    'best_val_loss': min_val_loss,\n",
    "    'best_psnr': best_psnr,\n",
    "    'config': config\n",
    "}\n",
    "history_path = os.path.join(config['pkl_dir'], 'training_history.pkl')\n",
    "with open(history_path, 'wb') as f:\n",
    "    pickle.dump(training_history, f)\n",
    "print(f\"Training history saved to: {history_path}\")\n",
    "\n",
    "print(\"\\n=== TRAINING COMPLETE ===\")\n",
    "print(f\"Best Validation Loss: {min_val_loss:.4f}\")\n",
    "print(f\"Best PSNR: {best_psnr:.4f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b253297",
   "metadata": {},
   "source": [
    "## 8. Testing Phase üß™\n",
    "\n",
    "**Why Testing Matters:**\n",
    "After training, we need to evaluate our model's performance on completely unseen data:\n",
    "\n",
    "**Testing vs Training/Validation:**\n",
    "- **Training**: Model sees this data and learns from it\n",
    "- **Validation**: Model sees this during training but doesn't learn from it\n",
    "- **Testing**: Model has NEVER seen this data - true performance measure\n",
    "\n",
    "**Key Metrics for Image Compression:**\n",
    "1. **PSNR (Peak Signal-to-Noise Ratio)**:\n",
    "   - Measures signal quality in decibels (dB)\n",
    "   - Higher = better (30+ dB is excellent)\n",
    "   - Most common metric in compression research\n",
    "\n",
    "2. **MS-SSIM (Multi-Scale Structural Similarity)**:\n",
    "   - Measures perceptual similarity (0 to 1)\n",
    "   - Closer to 1.0 = better perceptual quality\n",
    "   - Better aligned with human visual system than PSNR\n",
    "\n",
    "3. **BPP (Bits Per Pixel)** - if measured:\n",
    "   - Compression efficiency \n",
    "   - Lower = more compressed files\n",
    "\n",
    "**What the Results Tell Us:**\n",
    "- Individual image performance (some compress better than others)\n",
    "- Average performance across diverse scenes\n",
    "- Visual quality through saved image comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['test']:\n",
    "    results_path = os.path.join(config['save_output_path'], 'results')\n",
    "    if not os.path.exists(results_path):\n",
    "        os.makedirs(results_path)\n",
    "    \n",
    "    names = [\"Image Number\", \"PSNR\", \"MS-SSIM\"]\n",
    "    cols = dict()\n",
    "    model.eval()\n",
    "    mse_test = []\n",
    "    \n",
    "    print(f\"Testing on {len(test_loader)} images...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(iter(test_loader)):\n",
    "            img, cor_img, _, _ = data\n",
    "            img = img.cuda().float() if config['cuda'] else img.float()\n",
    "            cor_img = cor_img.cuda().float() if config['cuda'] else cor_img.float()\n",
    "            \n",
    "            # Forward pass\n",
    "            out_l, out_r = model(img, cor_img)\n",
    "            \n",
    "            x_recon = out_l\n",
    "            mse_dist = mse(img, x_recon)\n",
    "            mse_test.append(mse_dist.item())\n",
    "            msssim = 1 - ms_ssim(img.clone().cpu(), x_recon.clone().cpu(), data_range=1.0, \n",
    "                                 size_average=True, win_size=7)\n",
    "            msssim_db = -10 * np.log10(msssim)\n",
    "            \n",
    "            vals = [str(i)] + ['{:.8f}'.format(x) for x in [\n",
    "                10 * np.log10(1 / mse_dist.item()),\n",
    "                msssim.item()\n",
    "            ]]\n",
    "            \n",
    "            # Store results\n",
    "            for (name, val) in zip(names, vals):\n",
    "                if name not in cols:\n",
    "                    cols[name] = []\n",
    "                cols[name].append(val)\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1}/{len(test_loader)} images\")\n",
    "            \n",
    "            # Save images if requested\n",
    "            if config['save_image']:\n",
    "                save_image(x_recon[0], img[0], os.path.join(results_path, '{}_images'.format(1)), str(i))\n",
    "    \n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame.from_dict(cols)\n",
    "    csv_path = os.path.join(results_path, experiment_name + '.csv')\n",
    "    df.to_csv(csv_path)\n",
    "    \n",
    "    # Calculate and display average metrics\n",
    "    avg_psnr = np.mean([float(x) for x in cols['PSNR']])\n",
    "    avg_msssim = np.mean([float(x) for x in cols['MS-SSIM']])\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Average PSNR: {avg_psnr:.4f} dB\")\n",
    "    print(f\"Average MS-SSIM: {avg_msssim:.6f}\")\n",
    "    print(f\"Results saved to: {csv_path}\")\n",
    "    \n",
    "    # Display the results dataframe\n",
    "    display(df.head(10))\n",
    "else:\n",
    "    print(\"Testing skipped (config['test'] = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e787c35",
   "metadata": {},
   "source": [
    "## 9. Visualize Sample Results\n",
    "\n",
    "Display some sample reconstructed images from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample results\n",
    "if config['test']:\n",
    "    model.eval()\n",
    "    num_samples = 3  # Number of samples to visualize\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in range(num_samples):\n",
    "            # Get a sample from test dataset\n",
    "            img, cor_img, _, _ = test_dataset[idx]\n",
    "            img_batch = img.unsqueeze(0).cuda().float() if config['cuda'] else img.unsqueeze(0).float()\n",
    "            cor_img_batch = cor_img.unsqueeze(0).cuda().float() if config['cuda'] else cor_img.unsqueeze(0).float()\n",
    "            \n",
    "            # Get reconstruction\n",
    "            out_l, out_r = model(img_batch, cor_img_batch)\n",
    "            \n",
    "            # Convert to numpy for visualization\n",
    "            img_np = img.cpu().numpy().transpose(1, 2, 0)\n",
    "            cor_img_np = cor_img.cpu().numpy().transpose(1, 2, 0)\n",
    "            recon_np = out_l[0].cpu().numpy().transpose(1, 2, 0)\n",
    "            \n",
    "            # Clip values to [0, 1]\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            cor_img_np = np.clip(cor_img_np, 0, 1)\n",
    "            recon_np = np.clip(recon_np, 0, 1)\n",
    "            \n",
    "            # Plot\n",
    "            axes[idx, 0].imshow(img_np)\n",
    "            axes[idx, 0].set_title(f'Original Left Image {idx+1}')\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            axes[idx, 1].imshow(cor_img_np)\n",
    "            axes[idx, 1].set_title(f'Original Right Image {idx+1}')\n",
    "            axes[idx, 1].axis('off')\n",
    "            \n",
    "            axes[idx, 2].imshow(recon_np)\n",
    "            axes[idx, 2].set_title(f'Reconstructed Left Image {idx+1}')\n",
    "            axes[idx, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Sample visualizations complete!\")\n",
    "else:\n",
    "    print(\"Testing was not performed. Set config['test'] = True to visualize results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53e462",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Review the complete workflow and results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b9a91",
   "metadata": {},
   "source": [
    "## üéØ Learning Outcomes & Next Steps\n",
    "\n",
    "**What You've Learned:**\n",
    "‚úÖ **Deep Compression Theory**: Rate-distortion optimization in neural networks  \n",
    "‚úÖ **Stereo Processing**: Using geometric relationships for better compression  \n",
    "‚úÖ **Training Pipeline**: Complete deep learning workflow from scratch  \n",
    "‚úÖ **Performance Metrics**: PSNR, MS-SSIM, and BPP interpretation  \n",
    "‚úÖ **Practical Implementation**: Real-world compression system on KITTI dataset  \n",
    "\n",
    "**Key Takeaways:**\n",
    "- **Neural compression** learns patterns that traditional methods miss\n",
    "- **Stereo correlation** significantly improves compression efficiency  \n",
    "- **Rate-distortion trade-off** is controlled by lambda parameter\n",
    "- **Training from scratch** requires patience but provides deep understanding\n",
    "\n",
    "**Potential Improvements:**\n",
    "1. **Architecture**: Try different encoder/decoder designs\n",
    "2. **Loss Functions**: Experiment with perceptual losses\n",
    "3. **Dataset**: Train on more diverse image types\n",
    "4. **Optimization**: Advanced techniques like progressive training\n",
    "5. **Evaluation**: Add BPP measurement for complete analysis\n",
    "\n",
    "**Real-World Applications:**\n",
    "- üöó **Autonomous Vehicles**: Efficient stereo data transmission\n",
    "- üéÆ **VR/AR**: Real-time stereo compression for immersive experiences  \n",
    "- üì± **Mobile Devices**: Bandwidth-efficient video calling\n",
    "- üõ∞Ô∏è **Satellite Imaging**: Space-constrained data transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nExperiment Name: {experiment_name}\")\n",
    "print(f\"\\nDataset: {config['dataset_name']}\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  - Test samples: {len(test_dataset)}\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  - Baseline model: {config['baseline_model']}\")\n",
    "print(f\"  - Use side info: {config['use_side_info']}\")\n",
    "print(f\"  - Image size: {config['resize']}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  - Training enabled: {config['train']}\")\n",
    "print(f\"  - Lambda: {config['lambda']}\")\n",
    "print(f\"  - Learning rate: {config['lr']}\")\n",
    "print(f\"  - Batch size: {config['train_batch_size']}\")\n",
    "print(f\"  - Distortion loss: {config['distortion_loss']}\")\n",
    "print(f\"\\nTesting Configuration:\")\n",
    "print(f\"  - Testing enabled: {config['test']}\")\n",
    "print(f\"  - Save images: {config['save_image']}\")\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  - Save path: {config['save_output_path']}\")\n",
    "print(f\"  - Save weights: {config['save_weights']}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
